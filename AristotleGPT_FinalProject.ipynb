{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58970ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d382e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b80db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECTION 1\n",
      "\n",
      "Part 1 \n",
      "\n",
      "Things are said to be named 'equivocally' when, though they have\n",
      "a common name, the definition corresponding with the name differs\n",
      "for each. Thus, a real man and a figure in a picture can both lay\n",
      "claim to the name 'animal'; yet these are equivocally so named, for,\n",
      "though they have a common name, the definition corresponding with\n",
      "the name differs for each. For should any one define in what sense\n",
      "each is an animal, his definition in the one case will be appropriate\n",
      "to that case only. \n",
      "\n",
      "On the other hand, things are said to be named 'univocally' which\n",
      "have both the name and the definition answering to the name in common.\n",
      "A man and an ox are both 'animal', and these are univocally so named,\n",
      "inasmuch as not only the name, but also the definition, is the same\n",
      "in both cases: for if a man should state in what sense each is an\n",
      "animal, the statement in the one case would be identical with that\n",
      "in the other. \n",
      "\n",
      "Things are said to be named 'derivatively', which derive their nam\n"
     ]
    }
   ],
   "source": [
    "# List of Aristotle Works: https://classics.mit.edu/Browse/browse-Aristotle.html\n",
    "# Categories (81k)\n",
    "# On Dreams (23k)\n",
    "# On the Gait of Animals (43k)\n",
    "# On Generation and Corruption (152k)\n",
    "# On the Heavens (221k)\n",
    "# On Longevity and the Shortness of Life (14k)\n",
    "# On Memory and Reminiscence (27k)\n",
    "# Metaphysics (604k)\n",
    "# Nichomachean Ethics (456k)\n",
    "# Physics (455k)\n",
    "# Politics Book I (70k)\n",
    "# Posterior Analytics (189k)\n",
    "# Prior Analytics (255k)\n",
    "# Rhetoric (373k)\n",
    "# On The Soul (176k)\n",
    "# Topics (384k)\n",
    "\n",
    "# Reading in the Aristotle txt file\n",
    "with open('aristotle_texts.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663bebaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"&'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz|\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "# Determining the number of unique characters in the text \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fb4819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77, 55, 73, 73, 75, 70, 1, 56, 72, 69]\n",
      "wassup bro\n"
     ]
    }
   ],
   "source": [
    "# POSSIBLE ADDITION - USING SENTENCEPIECE OR TIKTOKEN for tokenization\n",
    "\n",
    "# create a mapping from characters to integers (trivial tokenization)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"wassup bro\"))\n",
    "print(decode(encode(\"wassup bro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e81b18e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3185699]) torch.int64\n",
      "tensor([45, 31, 29, 46, 35, 41, 40,  1, 14,  0,  0, 42, 55, 72, 74,  1, 14,  1,\n",
      "         0,  0, 46, 62, 63, 68, 61, 73,  1, 55, 72, 59,  1, 73, 55, 63, 58,  1,\n",
      "        74, 69,  1, 56, 59,  1, 68, 55, 67, 59, 58,  1,  5, 59, 71, 75, 63, 76,\n",
      "        69, 57, 55, 66, 66, 79,  5,  1, 77, 62, 59, 68,  9,  1, 74, 62, 69, 75,\n",
      "        61, 62,  1, 74, 62, 59, 79,  1, 62, 55, 76, 59,  0, 55,  1, 57, 69, 67,\n",
      "        67, 69, 68,  1, 68, 55, 67, 59,  9,  1])\n"
     ]
    }
   ],
   "source": [
    "# Encoding entire text into torch tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e60e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and validation\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70f8eb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([45, 31, 29, 46, 35, 41, 40,  1, 14])\n",
      "when input is tensor([45]) the target: 31\n",
      "when input is tensor([45, 31]) the target: 29\n",
      "when input is tensor([45, 31, 29]) the target: 46\n",
      "when input is tensor([45, 31, 29, 46]) the target: 35\n",
      "when input is tensor([45, 31, 29, 46, 35]) the target: 41\n",
      "when input is tensor([45, 31, 29, 46, 35, 41]) the target: 40\n",
      "when input is tensor([45, 31, 29, 46, 35, 41, 40]) the target: 1\n",
      "when input is tensor([45, 31, 29, 46, 35, 41, 40,  1]) the target: 14\n"
     ]
    }
   ],
   "source": [
    "context_window = 8\n",
    "print(train_data[:context_window+1])\n",
    "\n",
    "# Visualizing how text is split into different lengths, up to context size\n",
    "x = train_data[:context_window]\n",
    "y = train_data[1:context_window+1]\n",
    "for t in range(context_window):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49c998f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[77, 62, 55, 74,  9,  1, 69, 74],\n",
      "        [59, 68, 74, 24,  1, 55, 68, 58],\n",
      "        [73,  9,  1, 56, 75, 74,  0, 74],\n",
      "        [63, 57, 55, 56, 66, 59,  1, 69]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[62, 55, 74,  9,  1, 69, 74, 62],\n",
      "        [68, 74, 24,  1, 55, 68, 58,  0],\n",
      "        [ 9,  1, 56, 75, 74,  0, 74, 62],\n",
      "        [57, 55, 56, 66, 59,  1, 69, 60]])\n",
      "----\n",
      "when input is [77] the target: 62\n",
      "when input is [77, 62] the target: 55\n",
      "when input is [77, 62, 55] the target: 74\n",
      "when input is [77, 62, 55, 74] the target: 9\n",
      "when input is [77, 62, 55, 74, 9] the target: 1\n",
      "when input is [77, 62, 55, 74, 9, 1] the target: 69\n",
      "when input is [77, 62, 55, 74, 9, 1, 69] the target: 74\n",
      "when input is [77, 62, 55, 74, 9, 1, 69, 74] the target: 62\n",
      "when input is [59] the target: 68\n",
      "when input is [59, 68] the target: 74\n",
      "when input is [59, 68, 74] the target: 24\n",
      "when input is [59, 68, 74, 24] the target: 1\n",
      "when input is [59, 68, 74, 24, 1] the target: 55\n",
      "when input is [59, 68, 74, 24, 1, 55] the target: 68\n",
      "when input is [59, 68, 74, 24, 1, 55, 68] the target: 58\n",
      "when input is [59, 68, 74, 24, 1, 55, 68, 58] the target: 0\n",
      "when input is [73] the target: 9\n",
      "when input is [73, 9] the target: 1\n",
      "when input is [73, 9, 1] the target: 56\n",
      "when input is [73, 9, 1, 56] the target: 75\n",
      "when input is [73, 9, 1, 56, 75] the target: 74\n",
      "when input is [73, 9, 1, 56, 75, 74] the target: 0\n",
      "when input is [73, 9, 1, 56, 75, 74, 0] the target: 74\n",
      "when input is [73, 9, 1, 56, 75, 74, 0, 74] the target: 62\n",
      "when input is [63] the target: 57\n",
      "when input is [63, 57] the target: 55\n",
      "when input is [63, 57, 55] the target: 56\n",
      "when input is [63, 57, 55, 56] the target: 66\n",
      "when input is [63, 57, 55, 56, 66] the target: 59\n",
      "when input is [63, 57, 55, 56, 66, 59] the target: 1\n",
      "when input is [63, 57, 55, 56, 66, 59, 1] the target: 69\n",
      "when input is [63, 57, 55, 56, 66, 59, 1, 69] the target: 60\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "context_window = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_window, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_window] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_window+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(context_window): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b340c",
   "metadata": {},
   "source": [
    "### Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8581044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 82])\n",
      "tensor(4.9494, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "IJ'idsojTids(C\n",
      "4[a7P5T;t.aZesO:5El?yfr;\"ID C,)0bJk/u52m3j![a0:kAl?h?]V:fm1luBmgrOc70,8D(n&-1(r;za&&V\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7298c238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6bf1516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.361097812652588\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ec80cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "max_iters = 10000\n",
    "eval_interval = 500\n",
    "batch_size = 32\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be239bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.9093, val loss 4.8975\n",
      "step 500: train loss 4.2835, val loss 4.2875\n",
      "step 1000: train loss 3.7799, val loss 3.7985\n",
      "step 1500: train loss 3.3966, val loss 3.4138\n",
      "step 2000: train loss 3.1004, val loss 3.1337\n",
      "step 2500: train loss 2.8883, val loss 2.9324\n",
      "step 3000: train loss 2.7448, val loss 2.7778\n",
      "step 3500: train loss 2.6270, val loss 2.6856\n",
      "step 4000: train loss 2.5492, val loss 2.6070\n",
      "step 4500: train loss 2.5019, val loss 2.5533\n",
      "step 5000: train loss 2.4689, val loss 2.5149\n",
      "step 5500: train loss 2.4352, val loss 2.4999\n",
      "step 6000: train loss 2.4186, val loss 2.4703\n",
      "step 6500: train loss 2.4092, val loss 2.4700\n",
      "step 7000: train loss 2.3994, val loss 2.4551\n",
      "step 7500: train loss 2.3921, val loss 2.4466\n",
      "step 8000: train loss 2.3893, val loss 2.4392\n",
      "step 8500: train loss 2.3757, val loss 2.4380\n",
      "step 9000: train loss 2.3727, val loss 2.4338\n",
      "step 9500: train loss 2.3731, val loss 2.4313\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAByXklEQVR4nO3dd3gU1R7G8e9uyqaHJJQECBB6DVWqAgoIgghiQUSKoF5UVFQs2LFhuV6xgqKgCIoFrCgCUkR6LxKqQAIktEAChLTduX8sWVkSQghJJuX9PM88yZ45M/PbyZK8zJyZsRiGYSAiIiJSSljNLkBERESkICnciIiISKmicCMiIiKlisKNiIiIlCoKNyIiIlKqKNyIiIhIqaJwIyIiIqWKwo2IiIiUKgo3IiIiUqoo3EiOPvvsMywWi9tUoUIFOnfuzC+//JKtv8Vi4YUXXij6QgvR3r17Xe/9Qu9t2LBhrj4FqXPnznTu3Dlfy9aoUYOhQ4decP4LL7yQ7Web05Tf7Z/rcj4Xl7MPSoqsz9hnn312wT4PP/wwFouFbdu2XbDP008/jcViYd26dXne9vmfk7zUkiXrM5QfX375JePHj89xnlm/R7J+361Zs6bIty2Fw9PsAqR4mzJlCvXr18cwDBISEnj//ffp3bs3P/30E71793b1W758OVWrVjWx0sITGBjIZ599xnPPPYfV+u//B06dOsW3335LUFAQycnJJlZ4ae666y569Ojheh0fH0+/fv144IEHuP32213tQUFBl72ty/lcfPjhh5e9/dJg+PDhjB8/nsmTJ/PGG29km+9wOJg6dSrNmjWjRYsW+d5OREQEy5cvp1atWpdT7kV9+eWXbNmyhVGjRmWbV5p/j0jRUriRXDVu3JhWrVq5Xvfo0YOQkBC++uort3DTtm3bAt3umTNn8PX1LdB15lf//v355JNP+OOPP+jWrZur/euvv8Zut9O3b1+mTZtmYoWXpmrVqm5/QPbu3QtAtWrVcv05ZmRkYLFY8PTM+6+Ny/lcNGzYMN/LliaNGzemdevWfPHFF7z66qvZ9v/cuXPZv38/TzzxxGVtx2azFfi/40tl9val9NBpKbkkPj4+eHt74+Xl5dae0+Hkv/76i3bt2uHj40OVKlV49tln+eSTT7BYLK4/qOA8PH799dcza9Ysmjdvjo+PD2PHjgXggw8+oGPHjlSsWBF/f3+aNGnCG2+8QUZGhtu2OnfuTOPGjVm+fDnt27fH19eXGjVqMGXKFABmz55NixYt8PPzo0mTJsyZMyfP77levXq0b9+eyZMnu7VPnjyZfv36ERwcnG0Zh8PBG2+8Qf369bHZbFSsWJHBgwezf/9+t36GYfDGG29QvXp1fHx8aNGiBb/99luOdSQnJzN69GiioqLw9vamSpUqjBo1itOnT+f5veTVokWLsFgsfPHFFzz66KNUqVIFm83Grl27OHLkCPfddx8NGzYkICCAihUrcs0117BkyZJs6zn/c5F1+H/hwoXce++9lC9fnrCwMPr168fBgwfdlj3/tFTWaZP//ve//O9//yMqKoqAgADatWvHihUrsm170qRJ1K1bF5vNRsOGDfnyyy8ZOnQoNWrUuOj7//rrr7n22muJiIjA19eXBg0a8OSTT2bb10OHDiUgIIBdu3bRs2dPAgICiIyM5NFHHyUtLc2t78GDB7n11lsJDAwkODiY/v37k5CQcNFawHn0JiEhIcfPxpQpU7DZbAwcOJDU1FQeffRRmjVrRnBwMKGhobRr144ff/zxotu40Gmp2bNn06xZM2w2G1FRUfz3v//Ncfm8/Fvt3Lkzs2fPZt++fW6nQLPk9Htky5Yt9OnTh5CQEHx8fGjWrBmff/65W5+sz+tXX33F008/TeXKlQkKCqJr165s3779ou89r/766y+6dOlCYGAgfn5+tG/fntmzZ7v1SUlJcf079fHxITQ0lFatWvHVV1+5+vzzzz/cdtttVK5cGZvNRqVKlejSpQsbNmwosFrLOh25kVzZ7XYyMzMxDINDhw7x5ptvcvr0abfTFznZtGkT3bp1o27dunz++ef4+fkxceLECx7hWLduHTExMTzzzDNERUXh7+8PwO7du7n99ttdf9A3btzIK6+8wrZt27KFjYSEBO68804ef/xxqlatynvvvcewYcOIi4vju+++46mnniI4OJgXX3yRvn378s8//1C5cuU87Yfhw4dz//33c/z4cUJCQti+fTvLli3j5ZdfZubMmdn633vvvXz88ceMHDmS66+/nr179/Lss8+yaNEi1q1bR/ny5QEYO3YsY8eOZfjw4dx8883ExcVx9913Y7fbqVevnmt9KSkpdOrUif379/PUU08RHR3N33//zXPPPcfmzZuZP39+gY/7ARgzZgzt2rVj4sSJWK1WKlasyJEjRwB4/vnnCQ8P59SpU3z//fd07tyZP/74I0/jZO666y569erFl19+SVxcHI899hh33HEHCxYsuOiyH3zwAfXr13eN23j22Wfp2bMne/bscQXNjz/+mP/85z/cdNNNvP322yQlJTF27NhsgeNCdu7cSc+ePRk1ahT+/v5s27aN119/nVWrVmWrMSMjgxtuuIHhw4fz6KOP8ueff/LSSy8RHBzMc889BziPRHbt2pWDBw8ybtw46taty+zZs+nfv3+e6hkwYAAPP/wwkydPdjtievz4cX788UduvPFGQkJCSEpKIjExkdGjR1OlShXS09OZP38+/fr1Y8qUKQwePDhP28vyxx9/0KdPH9q1a8eMGTOw2+288cYbHDp0KFvfvPxb/fDDD7nnnnvYvXs333///UW3v337dtq3b0/FihV59913CQsLY9q0aQwdOpRDhw7x+OOPu/V/6qmn6NChA5988gnJyck88cQT9O7dm5iYGDw8PC7pvZ9v8eLFdOvWjejoaD799FNsNhsffvghvXv35quvvnL9LB955BG++OILXn75ZZo3b87p06fZsmULx44dc62rZ8+ern1ZrVo1jh49yrJlyzhx4sRl1SjnMERyMGXKFAPINtlsNuPDDz/M1h8wnn/+edfrW265xfD39zeOHDniarPb7UbDhg0NwNizZ4+rvXr16oaHh4exffv2XGuy2+1GRkaGMXXqVMPDw8NITEx0zevUqZMBGGvWrHG1HTt2zPDw8DB8fX2NAwcOuNo3bNhgAMa7776b6/b27NljAMabb75pnDx50ggICDDef/99wzAM47HHHjOioqIMh8Nh3H///ca5/5RiYmIMwLjvvvvc1rdy5UoDMJ566inDMAzj+PHjho+Pj3HjjTe69Vu6dKkBGJ06dXK1jRs3zrBarcbq1avd+n733XcGYPz666+uturVqxtDhgzJ9b1d6H1mWbhwoQEYHTt2vOjymZmZRkZGhtGlS5ds7+X8z0XW5+r8ffPGG28YgBEfH+9q69Spk9s+yKqzSZMmRmZmpqt91apVBmB89dVXhmE4Pyfh4eFGmzZt3Laxb98+w8vLy6hevfpF39O5HA6HkZGRYSxevNgAjI0bN7rmDRkyxACMb775xm2Znj17GvXq1XO9njBhggEYP/74o1u/u+++2wCMKVOmXLSOIUOGGF5eXsahQ4dcbe+9954BGPPmzctxmayfzfDhw43mzZu7zTv/c5K1f8+tpU2bNkblypWNM2fOuNqSk5ON0NBQI7c/H7n9W+3Vq9cFfwbnf15uu+02w2azGbGxsW79rrvuOsPPz884ceKEYRj/fl579uzp1u+bb74xAGP58uUXrNUw/v1cnv/v61xt27Y1KlasaJw8edLVlpmZaTRu3NioWrWq4XA4DMMwjMaNGxt9+/a94HqOHj1qAMb48eNzrUkuj05LSa6mTp3K6tWrWb16Nb/99htDhgzh/vvv5/333891ucWLF3PNNde4jlAAWK1Wbr311hz7R0dHU7du3Wzt69ev54YbbiAsLAwPDw+8vLwYPHgwdrudHTt2uPWNiIigZcuWrtehoaFUrFiRZs2auR2hadCgAQD79u27+A44KyAggFtuuYXJkyeTmZnJ1KlTufPOO3M8WrJw4UKAbFcstW7dmgYNGvDHH38AzsGTqampDBw40K1f+/btqV69ulvbL7/8QuPGjWnWrBmZmZmuqXv37lgsFhYtWpTn93IpbrrpphzbJ06cSIsWLfDx8cHT0xMvLy/++OMPYmJi8rTeG264we11dHQ0kLefSa9evdz+F37+stu3bychISHbZ61atWp06NAhT/X9888/3H777YSHh7s+d506dQLI9h4tFovb0ZSsms59LwsXLiQwMDDb+77YEdBzDR8+nIyMDL744gtX25QpU6hevTpdunRxtX377bd06NCBgIAA18/m008/zfPPJsvp06dZvXo1/fr1w8fHx9UeGBiY7f3Cpf1bzasFCxbQpUsXIiMj3dqHDh1KSkoKy5cvd2u/nM9Vbk6fPs3KlSu5+eabCQgIcLV7eHgwaNAg9u/f7zr91bp1a3777TeefPJJFi1axJkzZ9zWFRoaSq1atXjzzTf53//+x/r163E4HJdVn2SncCO5atCgAa1ataJVq1b06NGDjz76iGuvvZbHH38810Oox44do1KlStnac2oDZzA5X2xsLFdddRUHDhzgnXfeYcmSJaxevZoPPvgAIMdfGufz9vbO1u7t7Q1AamrqBevPyfDhw1m3bh2vvPIKR44cueDl1lmHn3N6T5UrV3bNz/oaHh6erd/5bYcOHWLTpk14eXm5TYGBgRiGwdGjRy/pveRVTu/hf//7H/feey9t2rRh5syZrFixgtWrV9OjR49sP5MLCQsLc3tts9mA7D/T/CybtV8v5fN3rlOnTnHVVVexcuVKXn75ZRYtWsTq1auZNWtWjjX6+fm5/fHPquncz9eF/j3k9LO/kKuuuoq6deu6xpFt2rSJdevWuYXsWbNmceutt1KlShWmTZvG8uXLWb16NcOGDbvkz/vx48dxOBx5+nxe6r/VvDp27NgF/x1lzT/X5XyucnP8+HEMw8hTLe+++y5PPPEEP/zwA1dffTWhoaH07duXnTt3As4w/Mcff9C9e3feeOMNWrRoQYUKFXjwwQc5efLkZdUp/9KYG7lk0dHR/P777+zYsYPWrVvn2CcsLCzH8/IXGkCZ0xGQH374gdOnTzNr1iy3IxlmDbrr0KED9erV48UXX6Rbt27Z/jeZJesXbHx8fLbLWg8ePOg6mpXVL6d9kpCQ4DbwtXz58vj6+mYbZ3Tu/MKQ089l2rRpdO7cmQkTJri1F5dfzFn79VI+f+dasGABBw8eZNGiRa6jNcBljYcICwtj1apV+arnXMOGDePJJ59k1apVfPnll1itVreQPW3aNKKiovj666/dfnZ5HWt0rpCQECwWywU/n+cqrH+rYWFhxMfHZ2vPGnxeWJ/784WEhGC1WvNUi7+/v2ss3aFDh1xHcXr37u26V1H16tX59NNPAdixYwfffPMNL7zwAunp6UycOLFI3lNppyM3csmyfmFVqFDhgn06derEggUL3I4oOBwOvv322zxvJ+uXc9b/vsB5ddGkSZMuseKC88wzz9C7d28effTRC/a55pprALINnl69ejUxMTGuUwht27bFx8eH6dOnu/VbtmxZtsPo119/Pbt37yYsLMx1JO3cKS9XABUUi8Xi9jMB51GE808RmKVevXqEh4fzzTffuLXHxsaybNmyiy6f0+cO4KOPPsp3TVdffTUnT57kp59+cmv/8ssvL2k9Q4YMwdPTk48++ojp06fTpUsXtzBhsVjw9vZ2CzYJCQl5ulrqfP7+/rRu3ZpZs2a5HfU5efIkP//8s1vfS/m3arPZ8nwkpUuXLq6wea6pU6fi5+dXZJeO+/v706ZNG2bNmuVWu8PhYNq0aVStWjXH0+qVKlVi6NChDBgwgO3bt5OSkpKtT926dXnmmWdo0qTJJd2EUXKnIzeSqy1btpCZmQk4D7vOmjWLefPmceONNxIVFXXB5Z5++ml+/vlnunTpwtNPP42vry8TJ050XUp77s3wLqRbt254e3szYMAAHn/8cVJTU5kwYQLHjx8vmDeXD3fccQd33HFHrn3q1avHPffcw3vvvYfVauW6665zXS0VGRnJww8/DDj/Nzh69Ghefvll7rrrLm655Rbi4uJ44YUXsh32HzVqFDNnzqRjx448/PDDREdH43A4iI2NZe7cuTz66KO0adOm0N73ua6//npeeuklnn/+eTp16sT27dt58cUXiYqKcn1WzGS1Whk7diz/+c9/uPnmmxk2bBgnTpxg7NixREREXPSz1759e0JCQhgxYgTPP/88Xl5eTJ8+nY0bN+a7psGDB/P2228zePBgXnnlFerUqcOvv/7K77//fknrCQ8Pp2fPnkyZMgXDMBg+fLjb/KxbKtx3332uq+9eeuklIiIiXKdFLsVLL71Ejx496NatG48++ih2u53XX38df39/EhMTXf0u5d9qkyZNmDVrFhMmTKBly5ZYrVa3e2md6/nnn+eXX37h6quv5rnnniM0NJTp06cze/Zs3njjjRxvw3A5FixY4Habiiw9e/Zk3LhxdOvWjauvvprRo0fj7e3Nhx9+yJYtW/jqq69cAa9NmzZcf/31REdHExISQkxMDF988QXt2rXDz8+PTZs2MXLkSG655Rbq1KmDt7c3CxYsYNOmTTz55JMF+n7KNFOHM0uxldPVUsHBwUazZs2M//3vf0Zqaqpbf867ysEwDGPJkiVGmzZtDJvNZoSHhxuPPfaY8frrrxuA6yoHw3BetdGrV68c6/j555+Npk2bGj4+PkaVKlWMxx57zPjtt98MwFi4cKGrX6dOnYxGjRplW/5C6waM+++/P9d9kNNVRDk5/2opw3BeLfL6668bdevWNby8vIzy5csbd9xxhxEXF+fWz+FwGOPGjTMiIyMNb29vIzo62vj555+zXSlkGIZx6tQp45lnnjHq1atneHt7G8HBwUaTJk2Mhx9+2EhISHB7zwV1tdS3336brX9aWpoxevRoo0qVKoaPj4/RokUL44cffjCGDBmS7SqY8z8XF7oqJWt75/9Mc7paKqefR06fv48//tioXbu24e3tbdStW9eYPHmy0adPn2xXDeVk2bJlRrt27Qw/Pz+jQoUKxl133WWsW7cu29VEQ4YMMfz9/bMt//zzz2f7TOzfv9+46aabjICAACMwMNC46aabjGXLluX5aqksP/74owEYoaGh2f4dGoZhvPbaa0aNGjUMm81mNGjQwJg0aVKO9eTlainDMIyffvrJiI6ONry9vY1q1aoZr732Wo7ry+u/1cTEROPmm282ypUrZ1gsFrf15PRz3Lx5s9G7d28jODjY8Pb2Npo2bZqtxgt9Xi/0ns53oatDs6asqzuXLFliXHPNNYa/v7/h6+trtG3b1vj555/d1vXkk08arVq1MkJCQgybzWbUrFnTePjhh42jR48ahmEYhw4dMoYOHWrUr1/f8Pf3NwICAozo6Gjj7bffdrsKUC6PxTAMo3Djk8i/rr32Wvbu3ZvvqydE8uvEiRPUrVuXvn378vHHH5tdjogUIp2WkkLzyCOP0Lx5cyIjI0lMTGT69OnMmzfPNZBOpLAkJCTwyiuvcPXVVxMWFsa+fft4++23OXnyJA899JDZ5YlIIVO4kUJjt9t57rnnSEhIwGKx0LBhQ7744ouLjlkRuVw2m429e/dy3333kZiY6Bp8OnHiRBo1amR2eSJSyHRaSkREREoVXQouIiIipYrCjYiIiJQqCjciIiJSqpS5AcUOh4ODBw8SGBiY463lRUREpPgxDIOTJ09SuXLli96Ms8yFm4MHD17wmUAiIiJSvMXFxWV7bt/5TA03L7zwAmPHjnVrq1SpUq4Pk1u8eDGPPPIIf//9N5UrV+bxxx9nxIgRed5mYGAg4Nw5QUFB+StcREREilRycjKRkZGuv+O5Mf3ITaNGjZg/f77rtYeHxwX77tmzh549e3L33Xczbdo0li5dyn333UeFChW46aab8rS9rFNRQUFBCjciIiIlTF6GlJgebjw9PbM9JPBCJk6cSLVq1Rg/fjwADRo0YM2aNfz3v//Nc7gRERGR0s30q6V27txJ5cqViYqK4rbbbuOff/65YN/ly5dz7bXXurV1796dNWvWkJGRkeMyaWlpJCcnu00iIiJSepkabtq0acPUqVP5/fffmTRpEgkJCbRv355jx47l2D8hIYFKlSq5tVWqVInMzEyOHj2a4zLjxo0jODjYNWkwsYiISOlm6mmp6667zvV9kyZNaNeuHbVq1eLzzz/nkUceyXGZ88+1ZT094kLn4MaMGeO2rqwBSSIikn92u/2CR8xF8svb2/uil3nnheljbs7l7+9PkyZN2LlzZ47zw8PDs11JdfjwYTw9PQkLC8txGZvNhs1mK/BaRUTKIsMwSEhI4MSJE2aXIqWQ1WolKioKb2/vy1pPsQo3aWlpxMTEcNVVV+U4v127dvz8889ubXPnzqVVq1Z4eXkVRYkiImVaVrCpWLEifn5+uhmqFJism+zGx8dTrVq1y/psmRpuRo8eTe/evalWrRqHDx/m5ZdfJjk5mSFDhgDOU0oHDhxg6tSpAIwYMYL333+fRx55hLvvvpvly5fz6aef8tVXX5n5NkREygS73e4KNhc6Wi5yOSpUqMDBgwfJzMy8rIMWpoab/fv3M2DAAI4ePUqFChVo27YtK1asoHr16gDEx8cTGxvr6h8VFcWvv/7Kww8/zAcffEDlypV59913dRm4iEgRyBpj4+fnZ3IlUlplnY6y2+2XFW4sRtaI3DIiOTmZ4OBgkpKSdBM/EZFLkJqayp49e4iKisLHx8fscqQUyu0zdil/v02/z42IiIhIQVK4ERERkVJF4UZERCQXFosl12no0KFmlyjnKVaXgpd0yakZ7D16muiq5cwuRURECkh8fLzr+6+//prnnnuO7du3u9p8fX3NKEtyoSM3BWTz/iRavDiPYZ+tweEoU2O0RURKtfDwcNcUHByMxWIhPDycSpUqceWVVzJp0iS3/lu2bMFqtbJ7927AeeRnwoQJXHfddfj6+hIVFcW3337rtsyBAwfo378/ISEhhIWF0adPH/bu3VtUb7HUUbgpIPUCU7nX82f+k/opfx/UwzlFREo7i8XCsGHDmDJlilv75MmTueqqq6hVq5ar7dlnn+Wmm25i48aN3HHHHQwYMICYmBgAUlJSuPrqqwkICODPP//kr7/+IiAggB49epCenl6k76m0ULgpIN720zxq/ZKhHr+z9O8LP9lcRERKjzvvvJPt27ezatUqwHkvoGnTpjFs2DC3frfccgt33XUXdevW5aWXXqJVq1a89957AMyYMQOr1conn3xCkyZNaNCgAVOmTCE2NpZFixYV9VsqFRRuCkpoTZL8a+BlsXNy61yzqxERkSIQERFBr169mDx5MgC//PILqamp3HLLLW792rVrl+111pGbtWvXsmvXLgIDAwkICCAgIIDQ0FBSU1Ndp7bk0mhAcQGy1usO6z4iKvEvTqQ8TDm/y3vwl4iIFH933XUXgwYN4u2332bKlCn0798/T3dxznp2ksPhoGXLlkyfPj1bnwoVKhR4vWWBjtwUoMDGvQDoZN3AnzsOm1yNiIgUhZ49e+Lv78+ECRP47bffsp2SAlixYkW21/Xr1wegRYsW7Ny5k4oVK1K7dm23KTg4uEjeQ2mjcFOQqrUjzepHBUsyuzcuMbsaEREpAh4eHgwdOpQxY8ZQu3btbKegAL799lsmT57Mjh07eP7551m1ahUjR44EYODAgZQvX54+ffqwZMkS9uzZw+LFi3nooYfYv39/Ub+dUkHhpiB5enOyaicA/Pf+oUvCRUTKiOHDh5Oenp7jURuAsWPHMmPGDKKjo/n888+ZPn06DRs2BJwPIv3zzz+pVq0a/fr1o0GDBgwbNowzZ87oGYj5pDE3Baxc0+sh9jfa2tew5WCSbugnIlKKDB06NMc7EsfHx+Pp6cngwYNzXK5y5crMnXvhi03Cw8P5/PPPC6rMMk9HbgqYZ71rAYi27mHV5hiTqxERkcKUlpbGrl27ePbZZ7n11lupVKmS2SUJCjcFL6Aix4IbA5C6dY7JxYiISGH66quvqFevHklJSbzxxhtmlyNn6bRUIfBu0ANWbKHWiaUcP51OiL8uCRcRKY0udJrqXIah8ZdFTUduCkFgE+cl4VdZN7Nk+wGTqxERESlbFG4KQ0QzTnmFEWBJ5cCGP8yuRkREpExRuCkMVisp1a4BIChugS4JFxERKUIKN4UktHlvANrZ17L5QJLJ1YiIiJQdCjeFxLPONWTiSU1rAhs2rDG7HBERkTJD4aaw2AI5GtYSAPt2XRIuIiJSVBRuCpHv2Qdp1k1eRuLpdJOrERGRgtK5c2dGjRpldhlyAQo3hSg4+noAWlu2sXzrXnOLEREpgywWS67Txe5RcyGzZs3ipZdeuqzahg4dSt++fS9rHZIz3cSvMIXVItEnktDUOA5tmANX1DW7IhGRMiU+Pt71/ddff81zzz3H9u3bXW2+vr5u/TMyMvDy8rroekNDQwuuSClwOnJTyNKiugIQenChLgkXESli4eHhrik4OBiLxeJ6nZqaSrly5fjmm2/o3LkzPj4+TJs2jWPHjjFgwACqVq2Kn58fTZo04auvvnJb7/mnpWrUqMGrr77KsGHDCAwMpFq1anz88ceXVfvixYtp3bo1NpuNiIgInnzySTIzM13zv/vuO5o0aYKvry9hYWF07dqV06dPA7Bo0SJat26Nv78/5cqVo0OHDuzbt++y6ilJFG4KWYUWNwDQwbGWTfuPm1yNiEjBMgyDlPTMIp8K8pEGTzzxBA8++CAxMTF0796d1NRUWrZsyS+//MKWLVu45557GDRoECtXrsx1PW+99RatWrVi/fr13Hfffdx7771s27YtXzUdOHCAnj17csUVV7Bx40YmTJjAp59+yssvvww4j0gNGDCAYcOGERMTw6JFi+jXrx+GYZCZmUnfvn3p1KkTmzZtYvny5dxzzz1YLJZ81VIS6bRUIfOMupJUiy8VSGLe2j9pVq2v2SWJiBSYMxl2Gj73e5Fvd+uL3fHzLpg/YaNGjaJfv35ubaNHj3Z9/8ADDzBnzhy+/fZb2rRpc8H19OzZk/vuuw9wBqa3336bRYsWUb9+/Uuu6cMPPyQyMpL3338fi8VC/fr1OXjwIE888QTPPfcc8fHxZGZm0q9fP6pXrw5AkyZNAEhMTCQpKYnrr7+eWrVqAdCgQYNLrqEk05GbwubpzZFKHQCw7JxrcjEiInK+Vq1aub222+288sorREdHExYWRkBAAHPnziU2NjbX9URHR7u+zzr9dfjw4XzVFBMTQ7t27dyOtnTo0IFTp06xf/9+mjZtSpcuXWjSpAm33HILkyZN4vhx59mB0NBQhg4dSvfu3enduzfvvPOO29ijskBHbopAUJNekDCfhqeWc+xUGmEBNrNLEhEpEL5eHmx9sbsp2y0o/v7+bq/feust3n77bcaPH0+TJk3w9/dn1KhRpKfnfkuP8wciWywWHA5HvmoyDCPbaaSsU3EWiwUPDw/mzZvHsmXLmDt3Lu+99x5PP/00K1euJCoqiilTpvDggw8yZ84cvv76a5555hnmzZtH27Zt81VPSaMjN0UgONp5v5um1n9YtTl/519FRIoji8WCn7dnkU+FOX5kyZIl9OnThzvuuIOmTZtSs2ZNdu7cWWjby0nDhg1ZtmyZ29iiZcuWERgYSJUqVQDnvu/QoQNjx45l/fr1eHt78/3337v6N2/enDFjxrBs2TIaN27Ml19+WaTvwUwKN0UhsBLx/s5zrsc3zja5GBERyU3t2rVdR0ViYmL4z3/+Q0JCQqFsKykpiQ0bNrhNsbGx3HfffcTFxfHAAw+wbds2fvzxR55//nkeeeQRrFYrK1eu5NVXX2XNmjXExsYya9Ysjhw5QoMGDdizZw9jxoxh+fLl7Nu3j7lz57Jjx44yNe5Gp6WKiL3WtbBpGxUTFmN3jMHDWnZGrYuIlCTPPvsse/bsoXv37vj5+XHPPffQt29fkpIK/iHIixYtonnz5m5tQ4YM4bPPPuPXX3/lscceo2nTpoSGhjJ8+HCeeeYZAIKCgvjzzz8ZP348ycnJVK9enbfeeovrrruOQ4cOsW3bNj7//HOOHTtGREQEI0eO5D//+U+B119cWYyCvJ6uBEhOTiY4OJikpCSCgoKKbLuZcWvw/LQLJw1fdg/dSLOoSkW2bRGRgpCamsqePXuIiorCx8fH7HKkFMrtM3Ypf791WqqIeFZpQZJHCIGWM+xaM9/sckREREothZuiYrWSGNEJAM9/5plcjIiISOmlcFOEQpo5H6QZfXoFx06lmVyNiIhI6aRwU4TKNe5OJh7UtMazdv0as8sREREplRRuipJPEAeCnKPiT23+1eRiRERESieFmyJmqeu8k2fE4T+x6ynhIiIiBU7hpohVvqIPAC2MrWzes9/kakREREofhZsi5lmxLoe9KmOzZBK7WqemRERECprCTVGzWDhR9RoAfPfofjciIiIFTeHGBBVa3ABAdOoqjp5MNbkaERG5mM6dOzNq1CjX6xo1ajB+/Phcl7FYLPzwww+Xve2CWk9ZonBjgpAGnTmDD5UsJ9i0+k+zyxERKbV69+5N165dc5y3fPlyLBYL69atu+T1rl69mnvuuedyy3Pzwgsv0KxZs2zt8fHxXHfddQW6rfN99tlnlCtXrlC3UZQUbszgaWN/SBsAzvz9m8nFiIiUXsOHD2fBggXs27cv27zJkyfTrFkzWrRoccnrrVChAn5+fgVR4kWFh4djs9mKZFulhcKNSTwb9ACg2jFdEi4iUliuv/56KlasyGeffebWnpKSwtdff83w4cM5duwYAwYMoGrVqvj5+dGkSRO++uqrXNd7/mmpnTt30rFjR3x8fGjYsCHz5mV/zM4TTzxB3bp18fPzo2bNmjz77LNkZGQAziMnY8eOZePGjVgsFiwWi6vm809Lbd68mWuuuQZfX1/CwsK45557OHXqlGv+0KFD6du3L//973+JiIggLCyM+++/37Wt/IiNjaVPnz4EBAQQFBTErbfeyqFDh1zzN27cyNVXX01gYCBBQUG0bNmSNWucN6vdt28fvXv3JiQkBH9/fxo1asSvvxbuBTWehbp2uaDI1n1g2RgaGbvZvGMXTevXMbskEZFLZxiQkVL02/XyA4vlot08PT0ZPHgwn332Gc899xyWs8t8++23pKenM3DgQFJSUmjZsiVPPPEEQUFBzJ49m0GDBlGzZk3atGlz0W04HA769etH+fLlWbFiBcnJyW7jc7IEBgby2WefUblyZTZv3szdd99NYGAgjz/+OP3792fLli3MmTOH+fOdF5sEBwdnW0dKSgo9evSgbdu2rF69msOHD3PXXXcxcuRItwC3cOFCIiIiWLhwIbt27aJ///40a9aMu++++6Lv53yGYdC3b1/8/f1ZvHgxmZmZ3HffffTv359FixYBMHDgQJo3b86ECRPw8PBgw4YNeHl5AXD//feTnp7On3/+ib+/P1u3biUgIOCS67gUCjcm8SxXhVhbHaql7SR+zU80rf+o2SWJiFy6jBR4tXLRb/epg+Dtn6euw4YN480332TRokVcffXVgPOUVL9+/QgJCSEkJITRo0e7+j/wwAPMmTOHb7/9Nk/hZv78+cTExLB3716qVq0KwKuvvpptnMwzzzzj+r5GjRo8+uijfP311zz++OP4+voSEBCAp6cn4eHhF9zW9OnTOXPmDFOnTsXf3/n+33//fXr37s3rr79OpUqVAAgJCeH999/Hw8OD+vXr06tXL/744498hZv58+ezadMm9uzZQ2RkJABffPEFjRo1YvXq1VxxxRXExsby2GOPUb9+fQDq1Pn3P+yxsbHcdNNNNGnSBICaNWtecg2XSqelTHSqWhcAAmIXmFyJiEjpVb9+fdq3b8/kyZMB2L17N0uWLGHYsGEA2O12XnnlFaKjowkLCyMgIIC5c+cSGxubp/XHxMRQrVo1V7ABaNeuXbZ+3333HVdeeSXh4eEEBATw7LPP5nkb526radOmrmAD0KFDBxwOB9u3b3e1NWrUCA8PD9friIgIDh8+fEnbOnebkZGRrmAD0LBhQ8qVK0dMTAwAjzzyCHfddRddu3bltddeY/fu3a6+Dz74IC+//DIdOnTg+eefZ9OmTfmq41LoyI2Jwq+4AXZOJDptLUdOnKJCucI9TCciUuC8/JxHUczY7iUYPnw4I0eO5IMPPmDKlClUr16dLl2c/8F86623ePvttxk/fjxNmjTB39+fUaNGkZ6enqd1G0b2cZOW806ZrVixgttuu42xY8fSvXt3goODmTFjBm+99dYlvQ/DMLKtO6dtZp0SOneew+G4pG1dbJvntr/wwgvcfvvtzJ49m99++43nn3+eGTNmcOONN3LXXXfRvXt3Zs+ezdy5cxk3bhxvvfUWDzzwQL7qyQsduTFRaO12nLAEE2Q5Q8zK380uR0Tk0lksztNDRT3lYbzNuW699VY8PDz48ssv+fzzz7nzzjtdf5iXLFlCnz59uOOOO2jatCk1a9Zk586deV53w4YNiY2N5eDBf0Pe8uXL3fosXbqU6tWr8/TTT9OqVSvq1KmT7Qoub29v7Hb7Rbe1YcMGTp8+7bZuq9VK3bp181zzpch6f3Fxca62rVu3kpSURIMGDVxtdevW5eGHH2bu3Ln069ePKVOmuOZFRkYyYsQIZs2axaOPPsqkSZMKpdYsCjdmslo5UL4DABnb5phcjIhI6RUQEED//v156qmnOHjwIEOHDnXNq127NvPmzWPZsmXExMTwn//8h4SEhDyvu2vXrtSrV4/BgwezceNGlixZwtNPP+3Wp3bt2sTGxjJjxgx2797Nu+++y/fff+/Wp0aNGuzZs4cNGzZw9OhR0tLSsm1r4MCB+Pj4MGTIELZs2cLChQt54IEHGDRokGu8TX7Z7XY2bNjgNm3dupWuXbsSHR3NwIEDWbduHatWrWLw4MF06tSJVq1acebMGUaOHMmiRYvYt28fS5cuZfXq1a7gM2rUKH7//Xf27NnDunXrWLBggVsoKgwKNybzadQTgKjjS3VJuIhIIRo+fDjHjx+na9euVKtWzdX+7LPP0qJFC7p3707nzp0JDw+nb9++eV6v1Wrl+++/Jy0tjdatW3PXXXfxyiuvuPXp06cPDz/8MCNHjqRZs2YsW7aMZ5991q3PTTfdRI8ePbj66qupUKFCjpej+/n58fvvv5OYmMgVV1zBzTffTJcuXXj//fcvbWfk4NSpUzRv3txt6tmzp+tS9JCQEDp27EjXrl2pWbMmX3/9NQAeHh4cO3aMwYMHU7duXW699Vauu+46xo4dCzhD0/3330+DBg3o0aMH9erV48MPP7zsenNjMXI6WViKJScnExwcTFJSEkFBQWaXQ+bp4xhv1sILO5tvWkSTJs3NLklEJEepqans2bOHqKgofHx8zC5HSqHcPmOX8ve72By5GTduHBaLJcd7A2RZtGiR6+ZG507btm0rukILmKd/CHt8nZfHHVn3s8nViIiIlHzF4mqp1atX8/HHHxMdHZ2n/tu3b3dLbRUqVCis0opEalRX2LqBcvsXAs+ZXY6IiEiJZvqRm1OnTjFw4EAmTZpESEhInpapWLEi4eHhrunca/lLoqqt+wLQKH0TR44dM7cYERGREs70cHP//ffTq1evCz61NSfNmzcnIiKCLl26sHDhwlz7pqWlkZyc7DYVN6HVGxNvDcdmyWTHitlmlyMiIlKimRpuZsyYwbp16xg3blye+kdERPDxxx8zc+ZMZs2aRb169ejSpQt//vnnBZcZN24cwcHBruncOywWGxYL8RU7Or/foUvCRaR4K2PXoUgRKqjPlmljbuLi4njooYeYO3dunkfd16tXj3r16rlet2vXjri4OP773//SsWPHHJcZM2YMjzzyiOt1cnJysQw4gdE9IeEb6iQtIzPTjqdnyT7VJiKlT9Zdb1NSUvD19TW5GimNsu4KfbnDTUwLN2vXruXw4cO0bNnS1Wa32/nzzz95//33SUtLy9Oba9u2LdOmTbvgfJvNhs1mK5CaC1PNVj04M9dGRY7z98ZlNGp5ldkliYi48fDwoFy5cq5nFPn5+V3wUQAil8rhcHDkyBH8/Pzw9Ly8eGJauOnSpQubN292a7vzzjupX78+TzzxRJ5T2/r164mIiCiMEouUh7cvO/1bEn16Gcc3/AwKNyJSDGU9sTq/D2EUyY3VaqVatWqXHZpNCzeBgYE0btzYrc3f35+wsDBX+5gxYzhw4ABTp04FYPz48dSoUYNGjRqRnp7OtGnTmDlzJjNnzizy+gtDRq1rYdMyyscvMrsUEZEcWSwWIiIiqFixIhkZGWaXI6WMt7c3VuvlDwcuFve5uZD4+Hi3x8Gnp6czevRoDhw4gK+vL40aNWL27Nn07NnTxCoLTo22N8KmF6ibsYMjh/ZToVJVs0sSEcmRh4dHib8Nh5ReevxCMbP7pebUsv/Dqmav0LrvSLPLERERKRZK5OMXxOlIRCcAPHbNM7kSERGRkknhppgp1/R6AOqcWkVmevbH3YuIiEjuFG6KmTrNO5NIIEGksGvdH2aXIyIiUuIo3BQzHp6e7ApqB0DyJj2KQURE5FIp3BRDRp1rAQhPWGxyJSIiIiWPwk0xVLtdHzINK9UccRyN2252OSIiIiWKwk0xFFa+Itu8GgIQu+IHc4sREREpYRRuiqnEKtcAYNsz3+RKREREShaFm2KqfIveANROWU/mmZMmVyMiIlJyKNwUU/Uat+IAFbCRwT+rfjW7HBERkRJD4aaY8vCwsrtcBwBS/la4ERERySuFm2LMs34PAKoc+QvK1iPARERE8k3hphir17YnKYaNCsZRjv2zzuxyRERESgSFm2IsrFwwW2xNATi46gdzixERESkhFG6KueTILgD479Ml4SIiInmhcFPMhbd0XhJeIzWGjJNHTK5GRESk+FO4KeYa1G/IDqpjxWDfyp/MLkdERKTYU7gp5jysFvaGXQVAesxvJlcjIiJS/CnclAA+Da8DIDJxGdgzTa5GRESkeFO4KQEat+5CohFAoHGaY9uWmF2OiIhIsaZwUwKEBvqyyecKAA6v/dHkakRERIo3hZsSIqVGVwDKx87R3YpFRERyoXBTQkS1v5lThg8VMuM5vesvs8sREREpthRuSoj61Sqx1Nv5IM2EJZ+bXI2IiEjxpXBTQlgsFs40uAWA8LhfISPV5IpERESKJ4WbEqRV594cMMLwN05zfINu6CciIpIThZsSpGpoACsDnAOLk1dMNbkaERGR4knhpoTxbD4AgKrHlsIpPWtKRETkfAo3JcxV7Tqw0VELDxwcXj7d7HJERESKHYWbEibE35stFZyPY3Bs+MrkakRERIofhZsSqHyb28kwPAg/vQ3j0FazyxERESlWFG5KoI7N6rOE5oDueSMiInI+hZsSyNfbg9jIGwDw2z4THHaTKxIRESk+FG5KqFodbiLJ8CM44wiZ//xpdjkiIiLFhsJNCdWubmXmWa8E4Mhfn5lbjIiISDGicFNCeXpYSap7EwCh++ZA2imTKxIRESkeFG5KsFYdurPHUQmbkUrq5h/NLkdERKRYULgpwaIjy7HI5xoAklZ8YXI1IiIixYPCTQlmsVgwom8FoMLRFZB80OSKREREzKdwU8Jd07YNKx31sWJwavWXZpcjIiJiOoWbEq5GeX9WB10LQMa66WAYJlckIiJiLoWbUiC01c2kGV6EnP4H4jeaXY6IiIipFG5KgWtb1We+oyUASSs1sFhERMo2hZtSoHyAje3h1wPguXUW2DNMrkhERMQ8CjelRM2213PECMI/IxFj1x9mlyMiImIahZtSolvjSH41nI9jOKF73oiISBmmcFNK+Ns8ORTVF4DAvXPhzAlT6xERETGLwk0p0qptJ7Y7quJppGPf8r3Z5YiIiJhC4aYUuapuReZ4dAbg5Krp5hYjIiJiEoWbUsTLw0pag5txGBbKHVkNiXvMLklERKTIKdyUMl3aNOUvR2MAMtbPMLkaERGRoqdwU8q0qBbCn77OJ4Wnr/tSj2MQEZEyR+GmlLFYLAQ2u5HThg3/07EQt8rskkRERIqUwk0p1KtVbeY4WgOQulZPChcRkbJF4aYUql0xkLXlugNg+XsmZKaZXJGIiEjRKTbhZty4cVgsFkaNGpVrv8WLF9OyZUt8fHyoWbMmEydOLJoCS5iarXpw0AjFlnkSdswxuxwREZEiUyzCzerVq/n444+Jjo7Otd+ePXvo2bMnV111FevXr+epp57iwQcfZObMmUVUaclxfbNIfrR3AODMat3zRkREyg7Tw82pU6cYOHAgkyZNIiQkJNe+EydOpFq1aowfP54GDRpw1113MWzYMP773/8WUbUlR3iwD/9Udj4p3HvvfDh91OSKREREiobp4eb++++nV69edO3a9aJ9ly9fzrXXXuvW1r17d9asWUNGRkaOy6SlpZGcnOw2lRVXXNGBTY4oPAw7xhYd3RIRkbLB1HAzY8YM1q1bx7hx4/LUPyEhgUqVKrm1VapUiczMTI4ezfnIxLhx4wgODnZNkZGRl113SdGjSTg/Gh0BOLNGp6ZERKRsMC3cxMXF8dBDDzFt2jR8fHzyvJzFYnF7bZy9Sd357VnGjBlDUlKSa4qLi8t/0SVMkI8XJ2vfQIbhgd+RjXBku9kliYiIFDrTws3atWs5fPgwLVu2xNPTE09PTxYvXsy7776Lp6cndrs92zLh4eEkJCS4tR0+fBhPT0/CwsJy3I7NZiMoKMhtKku6tGrMYodzoLZjgx7HICIipZ9p4aZLly5s3ryZDRs2uKZWrVoxcOBANmzYgIeHR7Zl2rVrx7x589za5s6dS6tWrfDy8iqq0kuUzvUq8NvZJ4VnrP8KHA5zCxIRESlkpoWbwMBAGjdu7Db5+/sTFhZG48bOBz+OGTOGwYMHu5YZMWIE+/bt45FHHiEmJobJkyfz6aefMnr0aLPeRrFn8/TAr3Evkg0/bCnxsO8vs0sSEREpVKZfLZWb+Ph4YmNjXa+joqL49ddfWbRoEc2aNeOll17i3Xff5aabbjKxyuKvV4ua/GJvC0Dmej2OQURESjeLYZStx0YnJycTHBxMUlJSmRl/43AY3DvuAz7KeJpMTz88H98F3v5mlyUiIpJnl/L3u1gfuZGCYbVaqNH8GvY5KuKZmQLbZptdkoiISKFRuCkj+javyveOKwHIWKdTUyIiUnop3JQRDSKC2FCuBwAeexdDcrzJFYmIiBQOhZsypHWrlqx21MWKAzZ/a3Y5IiIihULhpgy5oWllZtmvAiBj3XQoW2PJRUSkjFC4KUOqhviRUKUHaYYnXse2QcJms0sSEREpcAo3ZUzXlvWY72jhfLFRj2MQEZHSR+GmjOnVJIKfzj4pPHPjN2DPNLkiERGRgqVwU8aU8/OG2t04ZgTieeYI/LPQ7JJEREQKlMJNGdS7RTV+srcHwNjwlcnViIiIFCyFmzKoa4NKzDn7pHBj2y+QmmRuQSIiIgVI4aYM8vHyILJRe3Y6qmC1p8HWH80uSUREpMAo3JRRfZtXdd3zxrFBV02JiEjpoXBTRrWrFcYS36txGBassUvh+D6zSxIRESkQCjdllIfVQptm0SxzNHQ2bPrG3IJEREQKiMJNGda3WRW+d52a+lKPYxARkVJB4aYMa1wliG2hnUgxbFiP/wP715hdkoiIyGVTuCnDLBYL3ZvXYY7jCmfDJg0sFhGRkk/hpozr0+zfJ4U7Ns+EzDSTKxIREbk8CjdlXPUwf85UaU+CEYI19TjsmGN2SSIiIpdF4Ua4oXk1Zp49esPyD80tRkRE5DIp3Ai9oiP4wtGDNMMT4lbAvuVmlyQiIpJvCjdC+QAbDerU+ffozdLxptYjIiJyORRuBIDB7WowyX49DsPiHHdzaKvZJYmIiOSLwo0A0LleBWyV6v57Wfiyd80tSEREJJ8UbgRw3vPm3s61mJjZGwBj87dwIs7kqkRERC6dwo249GoSQWK5xiyzN8TiyITlH5hdkoiIyCVTuBEXTw8r93SsyQT7DQAY6z6HlESTqxIREbk0Cjfi5paWkWz1ackWRw0sGSmwapLZJYmIiFySfIWbuLg49u/f73q9atUqRo0axccff1xghYk5fL09uPPKKD7KvB4AY9VHkJ5iclUiIiJ5l69wc/vtt7Nw4UIAEhIS6NatG6tWreKpp57ixRdfLNACpegNaluDxZ7tiXVUwJJyDNZPM7skERGRPMtXuNmyZQutW7cG4JtvvqFx48YsW7aML7/8ks8++6wg6xMTBPt50b9NFB/bnUdvWPYe2DPMLUpERCSP8hVuMjIysNlsAMyfP58bbnAOQK1fvz7x8fEFV52YZviVNfmBzhw1giApFv7+3uySRERE8iRf4aZRo0ZMnDiRJUuWMG/ePHr06AHAwYMHCQsLK9ACxRzhwT70bB7FlEznz5al74BhmFuUiIhIHuQr3Lz++ut89NFHdO7cmQEDBtC0aVMAfvrpJ9fpKin57ulYi2mOrpwyfODQFtg13+ySRERELsozPwt17tyZo0ePkpycTEhIiKv9nnvuwc/Pr8CKE3PVrhhAu4a1+Wr7Ndzt+Sv89TbU6WZ2WSIiIrnK15GbM2fOkJaW5go2+/btY/z48Wzfvp2KFSsWaIFirhGda/Fp5nWkGx6wbynErTa7JBERkVzlK9z06dOHqVOnAnDixAnatGnDW2+9Rd++fZkwYUKBFijmahZZjqiadfnBfqWzYel4U+sRERG5mHyFm3Xr1nHVVVcB8N1331GpUiX27dvH1KlTefddPU26tLm3cy0+OntZuLFtNhzZYXJFIiIiF5avcJOSkkJgYCAAc+fOpV+/flitVtq2bcu+ffsKtEAx31V1yuMT0YC59pZYMGDZO2aXJCIickH5Cje1a9fmhx9+IC4ujt9//51rr70WgMOHDxMUFFSgBYr5LBYLIzrVYmJmbwCMjV9D0gGTqxIREclZvsLNc889x+jRo6lRowatW7emXbt2gPMoTvPmzQu0QCkermsczrHQZqx01MfiyIAVH5pdkoiISI4shpG/O7MlJCQQHx9P06ZNsVqdGWnVqlUEBQVRv379Ai2yICUnJxMcHExSUpKOMl2iaSv2Mf+nL/jM+00M7wAsD28B35CLLygiInKZLuXvd76O3ACEh4fTvHlzDh48yIEDzlMUrVu3LtbBRi7PzS2rssW3DTGOSCzpp2D1p2aXJCIikk2+wo3D4eDFF18kODiY6tWrU61aNcqVK8dLL72Ew+Eo6BqlmPDx8mDYVVF8lDX2ZuVEyDhjclUiIiLu8hVunn76ad5//31ee+011q9fz7p163j11Vd57733ePbZZwu6RilGBrapziKvK9lvlMdy+ghsmG52SSIiIm7yNeamcuXKTJw40fU08Cw//vgj9913n+s0VXGkMTeXb9xvMaT99SEveE2FkBowci145OtJHiIiInlS6GNuEhMTcxxbU79+fRITE/OzSilBhneI4nuuIdEIgON7IeZHs0sSERFxyVe4adq0Ke+//3629vfff5/o6OjLLkqKt4pBPvRsWYvPM7s7G/4aD/m76E5ERKTA5etcwhtvvEGvXr2YP38+7dq1w2KxsGzZMuLi4vj1118LukYphu7pWIt+q6/lP8Yv+CVsgn8WQq1rzC5LREQkf0duOnXqxI4dO7jxxhs5ceIEiYmJ9OvXj7///pspU6YUdI1SDEWV96d947p8be/sbPjrbVPrERERyZLvm/jlZOPGjbRo0QK73V5QqyxwGlBccDbvT2LE+9+z2PYwnhYH3L0QqrQwuywRESmFiuQmfiJNqgYTVbsBPzraOxuWjje1HhEREVC4kct0b+da/97Ub+tPcGy3yRWJiEhZp3Ajl6V9rTBslRvzh705FgxY9q7ZJYmISBl3SVdL9evXL9f5J06cuKSNT5gwgQkTJrB3714AGjVqxHPPPcd1112XY/9FixZx9dVXZ2uPiYnRM61MYrFYuLdzLSZ+2ZsuHusxNnyJpfMYCAw3uzQRESmjLincBAcHX3T+4MGD87y+qlWr8tprr1G7dm0APv/8c/r06cP69etp1KjRBZfbvn2722CiChUq5HmbUvC6NwrnzdAWrD1Zh5bshBUToNtYs8sSEZEyqkCvlioIoaGhvPnmmwwfPjzbvKwjN8ePH6dcuXL5Wr+uliocX62K5Y8fPuMT77cwbIFYHv4bfHIPwyIiInlVIq+WstvtzJgxg9OnT9OuXbtc+zZv3pyIiAi6dOnCwoULi6hCyU2/FlXY7NeWHY4qWNJOwhrd70hERMxherjZvHkzAQEB2Gw2RowYwffff0/Dhg1z7BsREcHHH3/MzJkzmTVrFvXq1aNLly78+eefF1x/WloaycnJbpMUPJunB8OuqsXH9usBMFZ8CBmpJlclIiJlkemnpdLT04mNjeXEiRPMnDmTTz75hMWLF18w4Jyvd+/eWCwWfvrppxznv/DCC4wdm338h05LFbyTqRl0fG0us42RVLYkQu93oOVQs8sSEZFSoESdlvL29qZ27dq0atWKcePG0bRpU9555508L9+2bVt27tx5wfljxowhKSnJNcXFxRVE2ZKDQB8vBrStxaeZzqvdjKXvgqP43q1aRERKJ9PDzfkMwyAtLS3P/devX09ERMQF59tsNoKCgtwmKTx3dohilqUrJwx/LIm7YdsvZpckIiJlTL6eCl5QnnrqKa677joiIyM5efIkM2bMYNGiRcyZMwdwHnU5cOAAU6dOBWD8+PHUqFGDRo0akZ6ezrRp05g5cyYzZ840823IOSoE2ujVqg5T13TjQc8f4K/x0OAGsFjMLk1ERMoIU8PNoUOHGDRoEPHx8QQHBxMdHc2cOXPo1q0bAPHx8cTGxrr6p6enM3r0aA4cOICvry+NGjVi9uzZ9OzZ06y3IDm456pa3LSyO/d4zMbn4DrYuwSiOppdloiIlBGmDyguarrPTdF44Kv1XPH3Kwz2nAe1roFB35tdkoiIlGAlakCxlE4jOtVkkr0ndsMCuxdA/EazSxIRkTJC4UYKRaPKwUTVacxsR1tnw9K8XwEnIiJyORRupNDc26kWEzN7A2D8/T0k7jG5IhERKQsUbqTQtK0ZinfVZiy2R2MxHLD8fbNLEhGRMkDhRgqNxWJhRKdaTLSfPXqzfhokHzS5KhERKe0UbqRQXduwEodCr2C1oy6WzFSYPRrK1gV6IiJSxBRupFBZrRZGdK7NMxnDyMQDts+GmJyfAyYiIlIQFG6k0PVtVoXkoLp8eHZwMb8+BmeOm1uUiIiUWgo3Uui8Pa2M6dmADzL78o9RGU4dgnnPmV2WiIiUUgo3UiR6R0fQomYET6Tf5WxYNxX2LDG3KBERKZUUbqRIWCwWXurbiA3WBkzP7OJs/PkhyDhjbmEiIlLqKNxIkaldMZDhV9bktcwBHCUEEnfD4jfMLktEREoZhRspUg92qU1gcChPpQ91Nix9BxI2m1qTiIiULgo3UqT8vD15rndD5jquYI6jNRh2+OkBcNjNLk1EREoJhRspct0bhdOxbgWeTR/CaYs/HFwPKyeaXZaIiJQSCjdS5CwWC2NvaESSRxgvpQ9wNi54GY7vNbUuEREpHRRuxBRR5f0Z0akmX9s7s87SCDJS4JeH9WgGERG5bAo3Ypr7rq5N1VB/Hk0dRqbFG3YvgE1fm12WiIiUcAo3YhofLw9e6N2IPUYE4zNudDbOGQOnj5pbmIiIlGgKN2KqLg0q0bVBRSZm9mKfZxScSXQGHBERkXxSuBHTPd+7ER6e3jxwehgGVtj8DeycZ3ZZIiJSQinciOkiQ/0YeXVtNhm1+Mra09n4y8OQdsrcwkREpERSuJFi4Z5ONYkq789LKf047h0BSXHOy8NFREQukcKNFAs2Tw9euKERZ/Dh4dNDnI0rJ8L+NeYWJiIiJY7CjRQbnepW4LrG4SyyR7PYpwtgwE8PQma62aWJiEgJonAjxcqz1zfEz9uDUSduIc07BA7/DcveMbssEREpQRRupFipXM6XB7vU4ThBvJQxyNm4+A04ssPcwkREpMRQuJFiZ1iHKGpXDGDamTbsCGwL9nT4+SFwOMwuTURESgCFGyl2vD2tvNinEWBh2NEB2D39IHYZrPvM7NJERKQEULiRYql9rfLc0LQy+40KTLbd4Wyc9zwkHzS3MBERKfYUbqTYeqZXAwJsnow71pGjwU0gLRl+fczsskREpJhTuJFiq2KQDw93q4sDK/cmD8GwesK2X2DrT2aXJiIixZjCjRRrQ9pVp354IKvPVGZh2O3Oxl9Hw5kTptYlIiLFl8KNFGueHlZe7tsYgHvjriE1qCacOgTznjO5MhERKa4UbqTYa1UjlJtbViUNb54z7nE2rvsc9v5lbmEiIlIsKdxIifDkdfUJ8vHkmyPV2F71JmfjTw9CRqq5hYmISLGjcCMlQvkAG491rwfAnfuvx+5fCRJ3w59vmFyZiIgUNwo3UmLc3qY6TaoEczDVxtSQkc7Gpe9AwmZzCxMRkWJF4UZKDA+rhZf6NsZigbG7apFYrQc4MuGnB8BhN7s8EREpJhRupERpFlmO266oBsD9x2/DsAXBwfWwcqLJlYmISHGhcCMlzuPd6xHi58XyI978FfWgs3HBy3B8n7mFiYhIsaBwIyVOiL83T/SoD8C9WxuRXqUdZKTATyPBnmFydSIiYjaFGymRbm0VSfNq5TiVbvCa1wjw9IU9f8Ivo8AwzC5PRERMpHAjJZLVauGlPo2xWmDyNi9iOowHixXWT4NF48wuT0RETKRwIyVW4yrBDGpbHYCR68LJ7PFf54zFr8OaKSZWJiIiZlK4kRLtkWvrUT7Am91HTvPxmU7Q8XHnjNmPwPY55hYnIiKmULiREi3Y14sx1zUA4O15O1hVYwQ0uwMMB3w7FPavMbdAEREpcgo3UuL1a1GFnk3CybAb/GfaWva1fwVqd4XMM/DlrXB0l9kliohIEVK4kRLPYrHw1i3NiK4azPGUDIZ9sYGk3p9ARDNIOQbT+sGpw2aXKSIiRUThRkoFX28PPhnciohgH3YfOc3I73aQcdvXEFIDTuyD6bdA2imzyxQRkSKgcCOlRsUgHz4Z0go/bw+W7DzKCwuOYAycCX5hEL8Bvh2im/yJiJQBCjdSqjSqHMw7tzXHYoHpK2OZss0Dbv8WvPxg13z4+SHd5E9EpJRTuJFSp1vDSjx19gqql2dvZcGpqnDLZ2DxgA3TYeEr5hYoIiKFSuFGSqW7roritisicRjwwJfr2RbUDq5/2znzzzdhzWRzCxQRkUKjcCOlksVi4cU+jWlXM4zT6XaGf7aGw3X7Q+cxzg6zH4Vtv5pbpIiIFAqFGym1vD2tTLijBTXL+3PgxBnumbqW1PajocVg503+vhsGcavMLlNERAqYqeFmwoQJREdHExQURFBQEO3ateO3337LdZnFixfTsmVLfHx8qFmzJhMnTiyiaqUkKufnzadDryDY14sNcScY/d0mjF7/gzrXnr3JX384utPsMkVEpACZGm6qVq3Ka6+9xpo1a1izZg3XXHMNffr04e+//86x/549e+jZsydXXXUV69ev56mnnuLBBx9k5syZRVy5lCRR5f2ZeEdLPK0WftkUz/gFe5wDjCu3gDOJzpv8nTxkdpkiIlJALIZRvK6LDQ0N5c0332T48OHZ5j3xxBP89NNPxMTEuNpGjBjBxo0bWb58eZ7Wn5ycTHBwMElJSQQFBRVY3VL8fb06lidmbgbgndua0ae2N0y+FhL/gfBouPNXsAWaXKWIiOTkUv5+F5sxN3a7nRkzZnD69GnatWuXY5/ly5dz7bXXurV1796dNWvWkJGR883Z0tLSSE5OdpukbOp/RTX+07EmAI99t4m1xzzgjpngVx4SNsE3g3WTPxGRUsD0cLN582YCAgKw2WyMGDGC77//noYNG+bYNyEhgUqVKrm1VapUiczMTI4ePZrjMuPGjSM4ONg1RUZGFvh7kJLj8R716dawEumZDu6ZupY4wmHgt+DlD7sXwE8P6CZ/IiIlnOnhpl69emzYsIEVK1Zw7733MmTIELZu3XrB/haLxe111lm189uzjBkzhqSkJNcUFxdXcMVLieNhtfDObc1oVDmIY6fTGf75ak6GNYFbP3fe5G/jV7DgJbPLFBGRy2B6uPH29qZ27dq0atWKcePG0bRpU955550c+4aHh5OQkODWdvjwYTw9PQkLC8txGZvN5roaK2uSss3P25NPhrSiYqCNHYdO8cBX68ms2QVueNfZYclbsGqSuUWKiEi+mR5uzmcYBmlpaTnOa9euHfPmzXNrmzt3Lq1atcLLy6soypNSIiLYl0+GtMLHy8qi7Ud4eXYMNL8Drn7a2eHXxyDmF3OLFBGRfDE13Dz11FMsWbKEvXv3snnzZp5++mkWLVrEwIEDAecppcGDB7v6jxgxgn379vHII48QExPD5MmT+fTTTxk9erRZb0FKsOiq5RjfvxkAny3by9Tle6HjY9ByKGDAzOEQu9LECkVEJD9MDTeHDh1i0KBB1KtXjy5durBy5UrmzJlDt27dAIiPjyc2NtbVPyoqil9//ZVFixbRrFkzXnrpJd59911uuukms96ClHA9GkfweI96AIz9eSuLdx6Fnm9B3esgMxW+6g9HdphcpYiIXIpid5+bwqb73Mj5DMNg9LebmLluP4E2T2bd1546IR7weW84sAaCq8Fd8yAw3OxSRUTKrBJ5nxsRs1gsFl7t15jWNUI5mZbJsM9XcyzdA27/BkJrQVIsTL8ZUnWPJBGRkkDhRgSweXowcVBLqoX6EZd4hv98sZY0WznnTf78K0LCZpjaB04dNrtUERG5CIUbkbNC/b2ZPPQKAn08WbPvOE/O3IwRUsN5kz/fUDi4Dj7pAoe3mV2qiIjkQuFG5By1KwYwYWBLPKwWvl9/gA8W7oLKzeCu+c5TVCdi4dNr4Z/FZpcqIiIXoHAjcp4r65Rn7A2NAPjv3B3M3hQPYbWcAadaO0hLcj5JfP10kysVEZGcKNyI5OCOttUZ1iEKgEe+2cCGuBPgFwqDfoDGN4MjE368Dxa8rGdRiYgUMwo3IhfwdK8GXFO/ImmZDu76fA0HTpwBLx/oNwmuOnvjyD/fhFl3Q2bOd9UWEZGip3AjcgEeVgvvDmhO/fBAjp5Ko9+HS1kXexysVujyLNzwPlg9YfO3MLUvpCSaXbKIiKBwI5KrAJsnk4deQZ2KARxKTuO2j1YwY9XZu2a3GAQDvwNbEMQug0+6wrHd5hYsIiIKNyIXU7mcL9/f34HujSqRbnfw5KzNPPPDZtIzHVDrahg+13kX48Td8Gk3PY9KRMRkCjcieRBg82TCwJY82q0uFgtMWxHL7ZNWcPhkKlRs4LySqnJzSDnmfGzDlplmlywiUmYp3IjkkdVq4YEudfh0SCsCbc4b/fV+7y/Wxx6HwEowdDbU6wX2NPhuGCz5n66kEhExgcKNyCW6pn4lfhzZgdpnx+H0/2gF36yOA29/6P8FtL3f2fGPsfDzg2DPMLdgEZEyRuFGJB9qVgjgh/s7cG1D5zicx2duco7DcVigx6vQ879gscK6qTD9FkhNMrtkEZEyQ+FGJJ8CbJ5MvKMlj5wzDmfgJ2fH4bS+G277Crz84Z+FMLkHnIgzu2QRkTJB4UbkMlitFh7sUodPBjvH4azee5wb3lvqvKNxvR5w568QEA6Htzofunlwvdkli4iUego3IgWgS4NK/DCyA7Uq+JOQnMqtE5c7x+FUbgZ3/wEVG8GpQzClJ2z71exyRURKNYUbkQJS6+w4nG7njMN57sctZARUhmFzoFYXyEiBGbfDiolmlysiUmop3IgUoEAfLz66oyUPd60LwNTl+xg4aSVHMmxw+9fQcihgwJwn4LcnwGE3tV4RkdJI4UakgFmtFh7q+u84nFV7E+n93l9sOHgarh8P3V50dlw5Eb6+A9JPm1qviEhpo3AjUki6NjxvHM5Hy/lm7X7o8BDc8jl4+sD2X2HKdXAywexyRURKDYUbkUKUNQ6na4NKpGc6ePy7TTz/4xYy6t8AQ34Gv/IQvxEmdYGYn3VHYxGRAqBwI1LIAn28+HhQS0Z1rQPA51njcMo1dT6TKqwOJO93nqL6uDPsnKeQIyJyGRRuRIqA1WphVNe6TBrcioCz43BueP8vNp4OgbsXQMfHwDsA4jfA9JthcnfY86fZZYuIlEgWwyhb/0VMTk4mODiYpKQkgoKCzC5HyqBdh09xzxdr+OfIabw9rbx6YxNublkVTh+FpeNh1STITHV2juoIVz8D1dqYWrOIiNku5e+3wo2ICZJTM3jk6w3MjzkMwJB21XmqVwNsnh7OwcVL/gdrp4A93blAnWvh6qegcnMTqxYRMY/CTS4UbqS4cDgM3vljJ+/8sROAepUCeePmaJpGlnN2OBEHf74J66eBcfZ+OPWvh6ufhkoNzSlaRMQkCje5ULiR4mb+1kM8MXMTx06nY7XAPR1rMaprHXy8PJwdju2GxW/Apq8BA7BA45ug8xgoX9vM0kVEiozCTS4UbqQ4Sjydzgs//c1PGw8CULOCP2/eHE3L6qH/djqyHRaNg7+/d762WKHpAOj0OITUKPqiRUSKkMJNLhRupDib+3cCz/ywhcMn07BYYGj7GjzWvR5+3p7/dkrYDAtfdd4AEMDqCS0Gw1WjIbiKOYWLiBQyhZtcKNxIcZeUksHLs7fy7dr9AFQL9eO1m5rQvlZ5947718LCV2D3H87XHja4Yjhc+TAEVCziqkVECpfCTS4UbqSkWLT9ME/N2szBJOdl4QPbVGNMzwYE2DzdO+5bBgtehn1Lna+9/KD1Pc7HPPiFIiJSGijc5ELhRkqSk6kZvPbbNqavjAWgSjlfXu3XhE51K7h3NAz4Z5Ez5BxY42zzDoR290O7+8AnuGgLFxEpYAo3uVC4kZJo2a6jPDFrE3GJZwC4pWVVnunVkGA/L/eOhgE7foeFLzvH5gD4lIP2D0CrYTqSIyIllsJNLhRupKRKSc/kjTnb+Xz5XgwDKgbaeOXGJnRrWCl7Z4cDYn5yXl11ZJuzzdMXmg2ANiOgQr2iLV5E5DIp3ORC4UZKutV7E3niu038c/Q0AH2aVeb53o0I9ffO3tlhhy2zYNk7/x7JAajdFdreC7W6gMVSRJWLiOSfwk0uFG6kNEjNsPP2/B1M+vMfHAaUD/DmxT6N6dkkIucFDMM58HjFh7BtNs6bAQLl6zlDTnR/8PYrsvpFRC6Vwk0uFG6kNNkQd4LHv9vIjkOnALiucTgv9mlMhUDbhRdK3AOrPoZ1X0D6SWebbwi0vBNa3w1BlYugchGRS6NwkwuFGylt0jLtvL9gFx8u2o3dYVDOz4vnezekb7MqWHI75ZSaDBumw4oJcGKfs83qCY1udB7NqdKyaN6AiEgeKNzkQuFGSqstB5J4/LtNbI1PBqBL/Yq8cmMTwoN9cl/QYYftvzlDzr6//m2PbOMMOfV7g4fnhZcXESkCCje5ULiR0izD7uCjxbt554+dZNgNAn08ebZXQ25pVTX3ozhZ4jfCiomw+VtwZDjbgiOdNwVsMRh8yxVq/SIiF6JwkwuFGykLdhw6yWPfbmTj/iQA2tYM5d7OtelYp3zeQs7JQ7DmU1j9KaQcdbZ5+UOz252Xkutp5CJSxBRucqFwI2VFpt3Bp3/t4a15O0jPdABQq4I/QztEcVOLKu4P47yQjFTnUZwVE+Dw3/+21+3hPGUV1UmXkotIkVC4yYXCjZQ1cYkpTF66h2/X7OdUWiYAQT6e3Na6GoPbVadqSB4uATcM2POnM+TsmIPrUvKKDZ0hp/HNupRcRAqVwk0uFG6krDqZmsF3a/fz2bK97DuWAoDVAtc2DOfODjVoHRWat1NWx3bDyomwfjpkOG8kiKcv1OwMdbs7j+oEXeB+OyIi+aRwkwuFGynrHA6DhdsPM2XpXv7addTV3qhyEHd2iKJ30whsnh4XX9GZE7D+C1g16d9LybNENIN61znDTkQznboSkcumcJMLhRuRf21POMlny/Ywa90B0s6Oyykf4M3tbapzR9tqVAy8yGXk4Dxldehv2PEbbJ8DB9biOm0FEBhx9ojOdRDVUaevRCRfFG5yoXAjkt3x0+l8tTqWL5bvIz4pFQAvDwvXR1fmzg41iK5aLu8rO3UYds513jtn98J/T13B2dNXnZynrup2192QRSTPFG5yoXAjcmEZdge//53AlKV7WbvvuKu9ZfUQ7uxQgx6NwvH0sF7CClNh71/OQcg75kBSnPv8iKbOIzpZp6+sl7BuESlTFG5yoXAjkjcb404wZekeZm+OJ8Pu/DVROdiHQe1qMKB1JOX8cngKeW5cp6/OBp39a3A7fRUQ7gw59a5zXmKu01cicg6Fm1wo3IhcmsPJqUxbsY/pK2M5djodAB8vKzc2r8qdHWpQt1Jg/lacdfpqxxzYteC801c+zoBTrwfU6Q7BVQrgnYhISaZwkwuFG5H8Sc2w8/PGg0xZutf1/CqAK2uX5+aWVWlfOyxvA5BzkpkGe5fAjt+dg5KTYt3nh9SAKq2gaivn1/Am4JXPbYlIiaRwkwuFG5HLYxgGq/YkMmXpXuZuTcBxzm+QepUCaV87jCtrl6dNzTACbPl44KZhwOGtzgHJO36H/atxO30FYPWC8MbugSesli45FynFFG5yoXAjUnDiElP4alUsi3ccYWt8Muf+NvG0WmgaWY4OtctzZe3yNIssh7dnPgYMnzkBB9fB/rVwYI1zrE7K0ez9fMpBlZb/hp0qLcE/LL9vTUSKGYWbXCjciBSOxNPpLN99jL92HWXZ7qOuuyBn8fP2oHVUKB1qladD7fLUDw/Eas3HkRbDcN40cP8a5z11Dqx1Ps08MzV73/NPZ0VEg6ctf29QRExVYsLNuHHjmDVrFtu2bcPX15f27dvz+uuvU69evQsus2jRIq6++ups7TExMdSvX/+i21S4ESkacYkpLN11lKW7j7Fs11HXYOQsYf7etKvlPIXVoXZ5IkMv4+ooewYc2vJv4Nm/Bo7tzN7P6uUcr5MVdqq2gtCaOp0lUgKUmHDTo0cPbrvtNq644goyMzN5+umn2bx5M1u3bsXf3z/HZbLCzfbt293eXIUKFfDwuPgt4xVuRIqew2GwLeEky3Yf5a9dR1n5TyJnMuxufaqH+dG+lvMUVrtaYYT6X+Kl5uc7cxwOrPv36M6FTmd5B4B/hbNTeefkVz7n135h4HmZdYlIvpSYcHO+I0eOULFiRRYvXkzHjh1z7JMVbo4fP065cuUueRsKNyLmS890sCHuBH/tOsrSXUfZEHcC+zkjky0WaBgRxJW1y9O+dnmaVg2+9PvqnO/801n71zhPZ9nTLm09PsHu4ccv7JwgVMH9tV8YeHhdXt0iApTgcLNr1y7q1KnD5s2bady4cY59ssJNjRo1SE1NpWHDhjzzzDM5nqoCSEtLIy3t319eycnJREZGKtyIFCMnUzNYtSfROV5n1zG2HzqZrU9EsA8NIoJoEBFIg4ggGkYEUT3MH4/8jNvJkpnuDDynj8LpI84jO6eP5vw65RgY9ouv83wB4VChHlRs4Pxaob5z8gvNf90iZVCJDDeGYdCnTx+OHz/OkiVLLthv+/bt/Pnnn7Rs2ZK0tDS++OILJk6cyKJFi3I82vPCCy8wduzYbO0KNyLF1+HkVJbtPsbSXUdZuSeR2MSUHPv5enlQLzwr7Di/1o8Iyt8l6BfjcEDqifOCzxE4fSznYHQmEQzHhdfnXzHn0ONfvuBrFykFSmS4uf/++5k9ezZ//fUXVatWvaRle/fujcVi4aeffso2T0duREq+5NQMtiecJCY+ma0Hk4mJT2b7oZOkZuQcHqqF+tEwIsjtSE/VEF8sRTlw2GGHlETnkaHDMXBkGxzZ7pzOv0nhufzKnw065wUf/woa+CxlWokLNw888AA//PADf/75J1FRUZe8/CuvvMK0adOIiYm5aF+NuREpHewOgz1HTxMTn3zOdJKE5BwuCQcCfTxpEO4MOw0rO4NP3UqB+Hhd/EKEApd2Eo7ucAadwzFnQ882ZxC6EN8QqHBO2KlYH8rXA59zf4+dE37cglA+2y1WPcxUio0SE24Mw+CBBx7g+++/Z9GiRdSpUydf67n55ptJTExkwYIFF+2rcCNSuiWeTneFna1nA8+uwyddD/88l9UCNSsE0DAiiEaVg2hY2TmWJyzApHvhpJ/+N/Qc2QaHtzm/Ht9Ltrs0FxWfcudcMVb+nIHT5XO+msyjEE4JilCCws19993Hl19+yY8//uh2b5vg4GB8fX0BGDNmDAcOHGDq1KkAjB8/nho1atCoUSPS09OZNm0ar732GjNnzqRfv34X3abCjUjZk57pYPeRU25HeLbGJ5N43r13slQKsp0NPMGuwFMt1C9/Nx0sCBln3ENP1hGf43tyH9djBt+QnIOP2+uzbb6hCkOSZ5fy99vUT9WECRMA6Ny5s1v7lClTGDp0KADx8fHExv57fjo9PZ3Ro0dz4MABfH19adSoEbNnz6Znz55FVbaIlDDentaz42/+/YVoGAaHT6ax9aDzCM/Ws+N59h47zaHkNA4lH2Hh9iOu/v7eHjRwO8ITTJ1KAUVzWsvLFyKaOqdzZaaDI+Pf127/VzUu0H4J8+yZzoHROV49doEB1GeOO6ecbqKYEw8b2ALA2995zyFv/xy+z3qdx366F1GZVyzG3BQlHbkRkdycSstke0Lyv6HnYDLbEk6Slpn9CImH1ULtCgE0rHw29JwNUCGXewPCkshhd4YaV/A5cl4QOu/KspRECu1Um4e3M+h4+TuDobcfeGVNvmfnnX3tfbbN1ffsV7f5532vexeZosScljKDwo2IXKpMu4N/jp52Czx/H0zieEpGjv0rB/u4TmfVqhhA+QAbFQJtVAiwUc7Pq2iv2iquHHZITXKOM3JNp875eiqH9pxen4K0s68v9YaM+WX1coYcq4dzILbF6pw453uL5ewg7fPbcupLzu0e3s4glfXV03ZeW07f2y4y39u5Hquns36rJ1jOfrVaz3vtcfY9epzX32rKlXsKN7lQuBGRgmAYBoeS0/j7YJLbqa3zHxh6Pk+rxRV2ygd4O0NPoM0tAJU/2xZo81QQuhT2DPfQk5EC6SnOr7l9f9F+ZyDjdPEb32Qmi8d54ei81wEV4T+LC3STJWbMjYhISWWxWAgP9iE82IcuDSq52k+mZrAt4SR/H0hia3wy+4+f4eipNI6cTON4SgaZDoOE5NQLXrJ+Lpun9d/Qc24ACrRR4WwwCvLxIsDHE3+bJ/7enpd3x+aSzsMLfMs5p4JmGJCZdk4gOuM8+mQ4AMP51cj6mtVm5NCWUz+H8wzduW0Ou3M8VWY62LOmjAt8f7H5Gc7az28z7Ge3k3nO9+e8zi3MGXaw253rKoYUbkREClCgjxdX1AjlihrZH6+Qnung2Ok0jp5M58ipVI6cTOPoqXSOnExzTVlB6GRaJmmZDg6cOMOBE2fyvH0/bw8CbJ4E2JyBJ+troI8n/jYP5/fnzAuwebrC0fnLlemgdD6LBbx8nBNl5NEZhnFe+Mn8NwC5vc50BqFzX1tNuH/UORRuRESKiLenlYhgXyKCfYHgXPumZtidgeeUe+hxC0Gn0jiZmsmp1Ewyzz54NCXdTkq6ncMnL3/8ia+XBwE+zjAU4OMehv5t8/r39Tn9An3+fW3zNPcPneSTxeK8VL8EXq5f8ioWESkDfLw8iAz1IzLU76J9DcMgLdPB6bRMTmVNqZmcTs/kZGomp9PsnE7L5GRaprNPaian0s/2OWeZrO+zbnh4JsPOmbMh63J4e1hzDEf+Nk98vTzw9XZOfud87+vlgZ+3Bz5eHvh5Z/Wz4nv2ez9vD2yeVo1Jkhwp3IiIlHAWiwUfL2cQKIi7K6dl2p0B6JyglPX9yXMC0clU9/kn0zI5lZpxTrhyPkU93e4g8XT6BW+amF8Wi/PokisgnROIfM+GH0+rFQ+rBU+rxfnV4+zX89utFjys1nPmn9eebXkLQT5elPPzJtTfm3J+XuY8ykNypHAjIiJubJ4e2AIuPyjZHQan088JP+eEoawjSakZdlLSMzmT7uBMRiZnzp5WO5Nh58x5X7Pa08/ec8gw/j0Nx+mCeOeXx8/bgxA/Z9BxBh5vQv3cA1Covzchft6E+HsT4ueFr5eHjj4VAoUbEREpFB5nj24E+RTsTe8y7Q5SMx2kpGeSmu4g5WwoOj8Ypdsd2B0GmQ4Du8Ph/GrPen1e+zlTtvn289sdZNgNks9kcDwlneMpGdgdxtmgdWkDwG2eVrew4/zei2BfL7w8rK4jR14e2Y8knX8Uyuv8o1Iezv7nvvawWvCyWrPdpibrpjDG2Rsr/vs6a75x3mvXkjn297BaqFUhIM/7oaAp3IiISIni6WElwMNKgK14/AkzDIPk1ExOnA06x0+nczzFeRruREoGiSnpnDj39dmv6XYHaZmOPN8aoCSpGGhj1dNdTdt+8fhkiIiIlFAWi4VgX+fRlupheVvGMAxOp9s5nkMAOp6SQfKZDDLcjjwZF32daXe4HXXK7bXdYbiO3liwnPP9v+/p3NdcaH4O7RYgxM/cR5Ao3IiIiBQxi8XiunossozcNqcoWc0uQERERKQgKdyIiIhIqaJwIyIiIqWKwo2IiIiUKgo3IiIiUqoo3IiIiEiponAjIiIipYrCjYiIiJQqCjciIiJSqijciIiISKmicCMiIiKlisKNiIiIlCoKNyIiIlKqKNyIiIhIqeJpdgFFzTAMAJKTk02uRERERPIq6+921t/x3JS5cHPy5EkAIiMjTa5ERERELtXJkycJDg7OtY/FyEsEKkUcDgcHDx4kMDAQi8VSoOtOTk4mMjKSuLg4goKCCnTdkjvte/No35tH+9482vdFzzAMTp48SeXKlbFacx9VU+aO3FitVqpWrVqo2wgKCtKH3STa9+bRvjeP9r15tO+L1sWO2GTRgGIREREpVRRuREREpFRRuClANpuN559/HpvNZnYpZY72vXm0782jfW8e7fvircwNKBYREZHSTUduREREpFRRuBEREZFSReFGREREShWFGxERESlVFG4KyIcffkhUVBQ+Pj60bNmSJUuWmF1SiTJu3DiuuOIKAgMDqVixIn379mX79u1ufQzD4IUXXqBy5cr4+vrSuXNn/v77b7c+aWlpPPDAA5QvXx5/f39uuOEG9u/f79bn+PHjDBo0iODgYIKDgxk0aBAnTpwo7LdYYowbNw6LxcKoUaNcbdr3hefAgQPccccdhIWF4efnR7NmzVi7dq1rvvZ94cjMzOSZZ54hKioKX19fatasyYsvvojD4XD10b4vwQy5bDNmzDC8vLyMSZMmGVu3bjUeeughw9/f39i3b5/ZpZUY3bt3N6ZMmWJs2bLF2LBhg9GrVy+jWrVqxqlTp1x9XnvtNSMwMNCYOXOmsXnzZqN///5GRESEkZyc7OozYsQIo0qVKsa8efOMdevWGVdffbXRtGlTIzMz09WnR48eRuPGjY1ly5YZy5YtMxo3bmxcf/31Rfp+i6tVq1YZNWrUMKKjo42HHnrI1a59XzgSExON6tWrG0OHDjVWrlxp7Nmzx5g/f76xa9cuVx/t+8Lx8ssvG2FhYcYvv/xi7Nmzx/j222+NgIAAY/z48a4+2vcll8JNAWjdurUxYsQIt7b69esbTz75pEkVlXyHDx82AGPx4sWGYRiGw+EwwsPDjddee83VJzU11QgODjYmTpxoGIZhnDhxwvDy8jJmzJjh6nPgwAHDarUac+bMMQzDMLZu3WoAxooVK1x9li9fbgDGtm3biuKtFVsnT5406tSpY8ybN8/o1KmTK9xo3xeeJ554wrjyyisvOF/7vvD06tXLGDZsmFtbv379jDvuuMMwDO37kk6npS5Teno6a9eu5dprr3Vrv/baa1m2bJlJVZV8SUlJAISGhgKwZ88eEhIS3PazzWajU6dOrv28du1aMjIy3PpUrlyZxo0bu/osX76c4OBg2rRp4+rTtm1bgoODy/zP6/7776dXr1507drVrV37vvD89NNPtGrViltuuYWKFSvSvHlzJk2a5JqvfV94rrzySv744w927NgBwMaNG/nrr7/o2bMnoH1f0pW5B2cWtKNHj2K326lUqZJbe6VKlUhISDCpqpLNMAweeeQRrrzySho3bgzg2pc57ed9+/a5+nh7exMSEpKtT9byCQkJVKxYMds2K1asWKZ/XjNmzGDdunWsXr062zzt+8Lzzz//MGHCBB555BGeeuopVq1axYMPPojNZmPw4MHa94XoiSeeICkpifr16+Ph4YHdbueVV15hwIABgD73JZ3CTQGxWCxurw3DyNYmeTNy5Eg2bdrEX3/9lW1efvbz+X1y6l+Wf15xcXE89NBDzJ07Fx8fnwv2074veA6Hg1atWvHqq68C0Lx5c/7++28mTJjA4MGDXf207wve119/zbRp0/jyyy9p1KgRGzZsYNSoUVSuXJkhQ4a4+mnfl0w6LXWZypcvj4eHR7YEfvjw4WyJXy7ugQce4KeffmLhwoVUrVrV1R4eHg6Q634ODw8nPT2d48eP59rn0KFD2bZ75MiRMvvzWrt2LYcPH6Zly5Z4enri6enJ4sWLeffdd/H09HTtF+37ghcREUHDhg3d2ho0aEBsbCygz31heuyxx3jyySe57bbbaNKkCYMGDeLhhx9m3LhxgPZ9Sadwc5m8vb1p2bIl8+bNc2ufN28e7du3N6mqkscwDEaOHMmsWbNYsGABUVFRbvOjoqIIDw9328/p6eksXrzYtZ9btmyJl5eXW5/4+Hi2bNni6tOuXTuSkpJYtWqVq8/KlStJSkoqsz+vLl26sHnzZjZs2OCaWrVqxcCBA9mwYQM1a9bUvi8kHTp0yHbLgx07dlC9enVAn/vClJKSgtXq/ifQw8PDdSm49n0JZ8Ig5lIn61LwTz/91Ni6dasxatQow9/f39i7d6/ZpZUY9957rxEcHGwsWrTIiI+Pd00pKSmuPq+99poRHBxszJo1y9i8ebMxYMCAHC/LrFq1qjF//nxj3bp1xjXXXJPjZZnR0dHG8uXLjeXLlxtNmjTRZZnnOfdqKcPQvi8sq1atMjw9PY1XXnnF2LlzpzF9+nTDz8/PmDZtmquP9n3hGDJkiFGlShXXpeCzZs0yypcvbzz++OOuPtr3JZfCTQH54IMPjOrVqxve3t5GixYtXJcwS94AOU5Tpkxx9XE4HMbzzz9vhIeHGzabzejYsaOxefNmt/WcOXPGGDlypBEaGmr4+voa119/vREbG+vW59ixY8bAgQONwMBAIzAw0Bg4cKBx/PjxIniXJcf54Ub7vvD8/PPPRuPGjQ2bzWbUr1/f+Pjjj93ma98XjuTkZOOhhx4yqlWrZvj4+Bg1a9Y0nn76aSMtLc3VR/u+5LIYhmGYeeRIREREpCBpzI2IiIiUKgo3IiIiUqoo3IiIiEiponAjIiIipYrCjYiIiJQqCjciIiJSqijciIiISKmicCMiZU6NGjUYP3682WWISCFRuBGRQjV06FD69u0LQOfOnRk1alSRbfuzzz6jXLly2dpXr17NPffcU2R1iEjR8jS7ABGRS5Weno63t3e+l69QoUIBViMixY2O3IhIkRg6dCiLFy/mnXfewWKxYLFY2Lt3LwBbt26lZ8+eBAQEUKlSJQYNGsTRo0ddy3bu3JmRI0fyyCOPUL58ebp16wbA//73P5o0aYK/vz+RkZHcd999nDp1CoBFixZx5513kpSU5NreCy+8AGQ/LRUbG0ufPn0ICAggKCiIW2+9lUOHDrnmv/DCCzRr1owvvviCGjVqEBwczG233cbJkycLd6eJSL4o3IhIkXjnnXdo164dd999N/Hx8cTHxxMZGUl8fDydOnWiWbNmrFmzhjlz5nDo0CFuvfVWt+U///xzPD09Wbp0KR999BEAVquVd999ly1btvD555+zYMECHn/8cQDat2/P+PHjCQoKcm1v9OjR2eoyDIO+ffuSmJjI4sWLmTdvHrt376Z///5u/Xbv3s0PP/zAL7/8wi+//MLixYt57bXXCmlvicjl0GkpESkSwcHBeHt74+fnR3h4uKt9woQJtGjRgldffdXVNnnyZCIjI9mxYwd169YFoHbt2rzxxhtu6zx3/E5UVBQvvfQS9957Lx9++CHe3t4EBwdjsVjctne++fPns2nTJvbs2UNkZCQAX3zxBY0aNWL16tVcccUVADgcDj777DMCAwMBGDRoEH/88QevvPLK5e0YESlwOnIjIqZau3YtCxcuJCAgwDXVr18fcB4tydKqVatsyy5cuJBu3bpRpUoVAgMDGTx4MMeOHeP06dN53n5MTAyRkZGuYAPQsGFDypUrR0xMjKutRo0armADEBERweHDhy/pvYpI0dCRGxExlcPhoHfv3rz++uvZ5kVERLi+9/f3d5u3b98+evbsyYgRI3jppZcIDQ3lr7/+Yvjw4WRkZOR5+4ZhYLFYLtru5eXlNt9iseBwOPK8HREpOgo3IlJkvL29sdvtbm0tWrRg5syZ1KhRA0/PvP9KWrNmDZmZmbz11ltYrc6D0N98881Ft3e+hg0bEhsbS1xcnOvozdatW0lKSqJBgwZ5rkdEig+dlhKRIlOjRg1WrlzJ3r17OXr0KA6Hg/vvv5/ExEQGDBjAqlWr+Oeff5g7dy7Dhg3LNZjUqlWLzMxM3nvvPf755x+++OILJk6cmG17p06d4o8//uDo0aOkpKRkW0/Xrl2Jjo5m4MCBrFu3jlWrVjF48GA6deqU46kwESn+FG5EpMiMHj0aDw8PGjZsSIUKFYiNjaVy5cosXboUu91O9+7dady4MQ899BDBwcGuIzI5adasGf/73/94/fXXady4MdOnT2fcuHFufdq3b8+IESPo378/FSpUyDYgGZynl3744QdCQkLo2LEjXbt2pWbNmnz99dcF/v5FpGhYDMMwzC5CREREpKDoyI2IiIiUKgo3IiIiUqoo3IiIiEiponAjIiIipYrCjYiIiJQqCjciIiJSqijciIiISKmicCMiIiKlisKNiIiIlCoKNyIiIlKqKNyIiIhIqaJwIyIiIqXK/wGNVO2EXHhS2QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "terese foss ed\n",
      "site ars thitheroranthelperanem\n",
      "\n",
      "o, isefised, th imomompls y sledenowigaly w an be tin pr thanissez. clexichincaron as\n",
      "of pe an, llacofo nothRbe bltsalit isureattho-ee by?) me peture\n",
      "evere pulyth t t twie nfothixtsl, (4key\n",
      "I\n",
      "po co be aytonaglashequg tal tty alls, phecatherecth thenuret\n",
      "ueld (Thingutont.\n",
      "ary torser be s ishovef if usl incts tey beang.ghe sd bl ciernmpper\n",
      "og on abed t tus) alerthalan ar, armownt to wessesive iby the tompare asedis go B. cle d d civis;\n",
      "Th uon, nnand\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Initialize lists to store iteration numbers and losses\n",
    "iters, train_losses, val_losses = [], [], []\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        # Append the current iteration and losses to the lists\n",
    "        iters.append(iter)\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "        \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Convert lists that may contain tensors to numpy arrays\n",
    "train_losses = np.array(train_losses)\n",
    "val_losses = np.array(val_losses)\n",
    "\n",
    "# If you originally had tensors in the list, ensure they're converted to numpy arrays or Python scalars\n",
    "train_losses = [loss.item() if hasattr(loss, 'item') else loss for loss in train_losses]\n",
    "val_losses = [loss.item() if hasattr(loss, 'item') else loss for loss in val_losses]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_losses = pd.DataFrame({'Iteration': iters, 'Train Loss': train_losses, 'Validation Loss': val_losses})\n",
    "\n",
    "# Melt the DataFrame to plot with seaborn lineplot\n",
    "df_melted = df_losses.melt('Iteration', var_name='Type', value_name='Loss')\n",
    "\n",
    "# Plotting\n",
    "sns.lineplot(data=df_melted, x='Iteration', y='Loss', hue='Type')\n",
    "plt.title('Bigram Model Training and Validation Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()  \n",
    "\n",
    "    \n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d204da8",
   "metadata": {},
   "source": [
    "### Math of Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4913e2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a13b0b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "        \n",
    "# version 2: using matrix multiply for a weighted aggregation (getting average or previous timestep indexes)\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)\n",
    "\n",
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n",
    "\n",
    "# Can do weighted aggregation (avg) of past elements by using triangular matrix multiplication tricks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41fe6de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q: What am I looking for\n",
    "# K: What do I contain\n",
    "# V: If you find me interesting, this is what I would communicate to you\n",
    "# Q,K,V matrix is learned - generated for each token input\n",
    "\n",
    "# Possible manipulation: Just give raw value x instead of linear layer v\n",
    "\n",
    "\"\"\"KEY INSIGHT: Instead of initializing wei with all zeros \n",
    "(equal attention to all prev tokens after softmax)\n",
    "we use Q, K, V to determine weights for wei before softmax \n",
    "\"\"\"\n",
    "\n",
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16 # head size is output size of Q,K,V\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape\n",
    "\n",
    "# Need to add scaling to control variance at initialization, otherwise softmax will get to 'peaky' and try to converge to largest value in a one-hot manner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67188ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiheaded attention: multiple single heads concatenated\n",
    "# Similar to doing convolution in groups\n",
    "# IDEA: Single head vs multihead attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bf8586c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add layer norms (normalizes the rows)\n",
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean\n",
    "        xvar = x.var(1, keepdim=True) # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape\n",
    "\n",
    "# Implement layer norm before layer (pre-norm), which is different from the original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c05144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout (at end of residual pathway, end of MHA, and after softmax in forward in self attention head layer)\n",
    "# reduce overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c4794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b4cef38",
   "metadata": {},
   "source": [
    "Run on RunPod PyTorch 2.1, 1xA100 80GB cluster w/ 80GB VRAM and 117 GB RAM, 12 vCPU ($1.89 / hr). Took 2 hours and 44 minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98cd9c0",
   "metadata": {},
   "source": [
    "## 10.8M model\n",
    "#### hyperparameters\n",
    "batch_size = 128 # how many independent sequences will we process in parallel?  \n",
    "block_size = 256 # what is the maximum context length for predictions?  \n",
    "max_iters = 2000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384 # Size of embedding for tokens (input size to linear layers)  \n",
    "n_head = 6 # number of heads in multi-headed attention  \n",
    "n_layer = 6 # number of decoder blocks stacked in network  \n",
    "dropout = 0.2  \n",
    "\n",
    "Output:   \n",
    "10.802002 M parameters\n",
    "step 0: train loss 4.3749, val loss 4.3671\n",
    "\n",
    "EMHaea9R&&.SE:GYL[)FDkCk|5aX77WkW1s!'HAENYb3iZ9Q+a,1G+ZaA+TY=7:DB.gtWVXQxngDQ!B]\"!n0i2 TNN\"8No:864RY  \n",
    "step 500: train loss 1.3870, val loss 1.3879\n",
    "\n",
    "'''''' by'' exqued'', thhen' in it, oneral'.\n",
    "Aganny them?g. first add in therefinition\n",
    "the two sency  \n",
    "step 1000: train loss 1.1152, val loss 1.1395\n",
    "\n",
    "exist, inhers we are train claise.\n",
    "\n",
    "Thus, whether wark as it is true, and that are inquality, we hal\n",
    "step 1500: train loss 1.0210, val loss 1.0731  \n",
    "\n",
    "thing to this important something seems to be the differentiated by\n",
    "the things both else] and in to \n",
    "step 1999: train loss 0.9693, val loss 1.0364  \n",
    "\n",
    "woth; indeed similarly sense them are also known and the same kind\n",
    "which means underlying the same, \n",
    "\n",
    "wether, or that which is scientific is either additing an attributable\n",
    "or at a universal series a, or in the units, since I mean a pre-less\n",
    "quantity resulting. For, that we have spatial: further, he does\n",
    "not exist present only what is being or badlely, but also why the termined\n",
    "could not. This will not exist also less, unless limit ought. \n",
    "\n",
    "When then the same is an instances of necessitate, while in the case\n",
    "being not-will cannot be a capable of like quantity, and come. For then\n",
    "it is capable of\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da34207e",
   "metadata": {},
   "source": [
    "## 14.3M model\n",
    "#### hyperparameters\n",
    "batch_size = 256 # how many independent sequences will we process in parallel?  \n",
    "block_size = 256 # what is the maximum context length for predictions? \n",
    "max_iters = 10000  \n",
    "eval_interval = 500  \n",
    "learning_rate = 3e-4  \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  \n",
    "eval_iters = 200  \n",
    "n_embd = 384 # Size of embedding for tokens (input size to linear layers)  \n",
    "n_head = 8 # number of heads in multi-headed attention  \n",
    "n_layer = 8 # number of decoder blocks stacked in network  \n",
    "dropout = 0.2  <br>\n",
    "\n",
    "Output <br>:  \n",
    "step 0: train loss 4.4358, val loss 4.4354\n",
    "\n",
    "O ];yrD]WL3UFx;H7QR:BXDx6m2eEO3,0IsnJ|UuP6\"X(uq'2 (ZfU19cw\"3|,(8ow6?5rTS|aKFn'\"b(pmITxMTLM|kd2 s=Y3O\n",
    "\n",
    "\n",
    "step 200: train loss 2.2986, val loss 2.3150\n",
    "\n",
    "B tany ati) ' ined theree ovif con idestton oplly, 'ssthitrem; buro t ledisede maf tar ' at plecapol\n",
    "\n",
    "\n",
    "step 400: train loss 1.4703, val loss 1.4708\n",
    "\n",
    "the scepesses to not, impers of the bond mone on elurst. Yould might leto, then, isinams of one\n",
    "we k\n",
    "\n",
    "\n",
    "step 600: train loss 1.2005, val loss 1.2097\n",
    "\n",
    "and wight, if is a matter from itsel= are present, and consequently\n",
    "(3) and that which is they numbe\n",
    "\n",
    "\n",
    "step 800: train loss 1.0938, val loss 1.1220\n",
    "\n",
    "Epwardly. The tpeople proably hapbe in ricles\n",
    "to be the definition of lipse, supposit we dill\n",
    "with o\n",
    "\n",
    "\n",
    "step 1000: train loss 1.0361, val loss 1.0816\n",
    "\n",
    "\n",
    "\"\"(Or obtail is so, were altered with these. Sometions too 'the stars',\n",
    "but which they show they ma\n",
    "\n",
    "\n",
    "step 1200: train loss 0.9978, val loss 1.0523\n",
    "\n",
    "something heads only: It is said; (b) a\n",
    "man what begans (ii) he will be think by setter. But, in ano\n",
    "\n",
    "\n",
    "step 1400: train loss 0.9583, val loss 1.0256\n",
    "\n",
    "less can be potentially, while something else it must. But\n",
    "any other partness in argument in really \n",
    "\n",
    "\n",
    "step 1600: train loss 0.9338, val loss 1.0151\n",
    "\n",
    "seful of seeing that is not the absence of thing); for if a matter\n",
    "could be motion from one may be t\n",
    "\n",
    "\n",
    "step 1800: train loss 0.9049, val loss 0.9937\n",
    "\n",
    "are not more identical, or beyond the same thing. Again, if nothing\n",
    "as moveer by itself but the move\n",
    "\n",
    "\n",
    "step 2000: train loss 0.8862, val loss 0.9900\n",
    "\n",
    "He has are taken absurd to subject, a number in which he marks\n",
    "a circle or a simple wholly once. For\n",
    "\n",
    "\n",
    "step 2200: train loss 0.8570, val loss 0.9671\n",
    "\n",
    "will belong to the fact that are flesh to the science of all\n",
    "the causes, yet in the same way. That w\n",
    "\n",
    "\n",
    "step 2400: train loss 0.8350, val loss 0.9602\n",
    "\n",
    "'poletical' leadinness against give relative step, in\n",
    "another, thoughs they aime at \"at the hand'. I\n",
    "\n",
    "\n",
    "step 2600: train loss 0.8184, val loss 0.9538\n",
    "\n",
    "the things itself are obsercts, and are not obviously conscious'\n",
    "for all having their forms of leave\n",
    "\n",
    "\n",
    "step 2800: train loss 0.8025, val loss 0.9550\n",
    "\n",
    "is in time: so in the way that it is not possible for a thing\n",
    "in the wholesol, and changes. And, som\n",
    "\n",
    "\n",
    "step 3000: train loss 0.7886, val loss 0.9537\n",
    "\n",
    "for his faculty with the sight of hnorator man. But we must\n",
    "take their discovery arguments apart fro\n",
    "\n",
    "\n",
    "step 3200: train loss 0.7693, val loss 0.9468\n",
    "\n",
    "in what lies causes suffering, and gain water in flesh in Athenias the family\n",
    "of touce hunds: for th\n",
    "\n",
    "\n",
    "step 3400: train loss 0.7539, val loss 0.9488\n",
    "\n",
    "It clearly then that\n",
    "in no paositic syllogism that when one or not the other, and that\n",
    "A belongs to \n",
    "\n",
    "\n",
    "step 3600: train loss 0.7395, val loss 0.9466\n",
    "\n",
    "the reverse has been, so that if there is an infinity; nothing which\n",
    "has parts of the power of cause\n",
    "\n",
    "\n",
    "step 3800: train loss 0.7224, val loss 0.9475\n",
    "\n",
    "Since white produces no white and black, is ultimate thing to be\n",
    "the thine and the incontinent. Ther\n",
    "\n",
    "\n",
    "step 4000: train loss 0.7098, val loss 0.9474\n",
    "\n",
    "substance that has matter, in a sense the other category or substance\n",
    "for another, but it is hether \n",
    "\n",
    "\n",
    "step 4200: train loss 0.6957, val loss 0.9497\n",
    "\n",
    "\n",
    "\"Oresons, there are some who make regular our power on people\n",
    "do not in advocate increasing itsimpl\n",
    "\n",
    "\n",
    "step 4400: train loss 0.6835, val loss 0.9562\n",
    "\n",
    "means only what will happen, as we are said, considering self,\n",
    "with hot and cold, or moists, are, wh\n",
    "\n",
    "\n",
    "step 4600: train loss 0.6688, val loss 0.9595\n",
    "\n",
    "the things of a siolong seem, and in nature to generates? Surely,\n",
    "for a brazen specific death is the\n",
    "\n",
    "\n",
    "step 4800: train loss 0.6539, val loss 0.9595\n",
    "\n",
    "action. And now (b) the incontinent man is being productive in matter,\n",
    "but no name is qualification \n",
    "\n",
    "\n",
    "step 5000: train loss 0.6449, val loss 0.9630\n",
    "\n",
    "that is to which something else direction will be generated. Further,\n",
    "in either of the whole is some\n",
    "\n",
    "\n",
    "step 5200: train loss 0.6294, val loss 0.9700\n",
    "\n",
    "from which we reduced? Again, as has been mentioned, one musician,\n",
    "it is mind-this problem follows, \n",
    "\n",
    "\n",
    "step 5400: train loss 0.6205, val loss 0.9760\n",
    "\n",
    "coming arise at a line which by nature, or line, or in point; for\n",
    "it is a line, and if it is not pos\n",
    "\n",
    "\n",
    "step 5600: train loss 0.6049, val loss 0.9778\n",
    "\n",
    "elementary way should be knowingly-two: but without qualification,\n",
    "necessarily view there is knowled\n",
    "\n",
    "\n",
    "step 5800: train loss 0.5935, val loss 0.9824\n",
    "\n",
    "theme voluntary; since the lower enjoyment is bad, yet, if from he\n",
    "handed both their production he w\n",
    "\n",
    "\n",
    "step 6000: train loss 0.5833, val loss 0.9956\n",
    "\n",
    "leading to your reasoning that to be moved by something else, but\n",
    "to those that they do do so. Howev\n",
    "\n",
    "\n",
    "step 6200: train loss 0.5708, val loss 0.9953\n",
    "\n",
    "is an action; but no man does anything accidentally by 'not being\n",
    "a but 'being a man' or 'not-being \n",
    "\n",
    "\n",
    "step 6400: train loss 0.5598, val loss 1.0029\n",
    "\n",
    "for dangers against the law that wherever' is voluntary, whereas\n",
    "young people do not really wivole u\n",
    "\n",
    "\n",
    "step 6600: train loss 0.5469, val loss 1.0128\n",
    "\n",
    "for one class, the den air, and the dense of wood and everything else\n",
    "and contrary. It is clear how \n",
    "\n",
    "\n",
    "step 6800: train loss 0.5359, val loss 1.0135\n",
    "\n",
    "of seeing, we plain that it is needed not to both; for we make it\n",
    "in the case of unequals whether so\n",
    "\n",
    "\n",
    "step 7000: train loss 0.5277, val loss 1.0237\n",
    "\n",
    "that movement is the series of the terms, which is equally of a straight\n",
    "line certainly does not alw\n",
    "\n",
    "\n",
    "step 7200: train loss 0.5172, val loss 1.0260\n",
    "\n",
    "at the same time both because they cannot believe, indeed, but in\n",
    "every case they enjoy; for things \n",
    "\n",
    "\n",
    "step 7400: train loss 0.5044, val loss 1.0314\n",
    "\n",
    "them must be the same for demonstration. If, on the other hand, the\n",
    "negative conclusion does not dep\n",
    "\n",
    "\n",
    "step 7600: train loss 0.4975, val loss 1.0417\n",
    "\n",
    "of their existence before every movement in so far as it is according\n",
    "to the soul by some movement, \n",
    "\n",
    "\n",
    "step 7800: train loss 0.4865, val loss 1.0513\n",
    "\n",
    "The universal last is universal: after the negative demonstration\n",
    "is possible, the affirmative or th\n",
    "\n",
    "\n",
    "step 8000: train loss 0.4740, val loss 1.0508\n",
    "\n",
    "of things combined with the precision like themselves, as 'the prime\n",
    "end' of our body is taken from \n",
    "\n",
    "\n",
    "step 8200: train loss 0.4653, val loss 1.0637\n",
    "\n",
    "contrary to the other types: this is, for example and the contraries\n",
    "above writers. But perhaps thos\n",
    "\n",
    "\n",
    "step 8400: train loss 0.4578, val loss 1.0715\n",
    "\n",
    "some time B may be left as one described from A. Suppose that A and\n",
    "B belong to all the original B s\n",
    "\n",
    "\n",
    "step 8600: train loss 0.4484, val loss 1.0783\n",
    "\n",
    "very more uable and are always alike the same. But, as from an animal\n",
    "is not a hand, so there is no \n",
    "\n",
    "\n",
    "step 8800: train loss 0.4401, val loss 1.0783\n",
    "\n",
    "proves the point. Now these cannot be the source of animate not continuous\n",
    "extent from also-animal f\n",
    "\n",
    "\n",
    "step 9000: train loss 0.4322, val loss 1.0916\n",
    "\n",
    "man would rold on friend omney or the other, the hindrance of the\n",
    "disposition of the state. And ther\n",
    "\n",
    "\n",
    "step 9200: train loss 0.4236, val loss 1.1002\n",
    "\n",
    "paractical wisdom; nor can all Medea the house. For he that of a\n",
    "cilas. Even the answer that a sign \n",
    "\n",
    "\n",
    "step 9400: train loss 0.4143, val loss 1.1076\n",
    "\n",
    "seems to be that of the body which is due to the character of the\n",
    "force. And it is these functions t\n",
    "\n",
    "\n",
    "step 9600: train loss 0.4087, val loss 1.1090\n",
    "\n",
    "obtains, on that ground the fact that, appetite is the thing to which\n",
    "the virtue and will depart mov\n",
    "\n",
    "\n",
    "step 9800: train loss 0.4035, val loss 1.1234\n",
    "\n",
    "gather, psie, according to the matterials of certain speeches. We can\n",
    "now turn out that no sensible \n",
    "\n",
    "\n",
    "step 9999: train loss 0.3922, val loss 1.1283\n",
    "\n",
    "lack of being and being, and theoretical sciences use the same as\n",
    "familiar or discriminating and tho\n",
    "\n",
    "\n",
    "\n",
    "so though that things out of the same results a difference.\n",
    "\n",
    "(1) Between animals makes things past exist in sensible things, that\n",
    "we are seeking do them: an existence of things is sometime. Thus Just\n",
    "as to say the strength is a cube, is nearly substance that first sometimes\n",
    "not substance, when a separable thing is fixed by pleasure, but not\n",
    "by further-we must not say that suming all things that we do not necessaryly\n",
    "find everything that wants more permanent to a less. The first cause\n",
    "of the good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80ee20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
